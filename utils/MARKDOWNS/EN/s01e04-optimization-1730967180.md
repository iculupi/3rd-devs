![](https://cloud.overment.com/S01E04-1730930695.png)

<div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1026693454?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="S01E04 Optimization Techniques"></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

Application code optimization is important but often overlooked, as it usually doesn't play a key role in small projects. In the case of prompts, it is somewhat different because even simple tasks can be incorrectly executed due to a lack of precision. Interestingly, this refers not only to the precision of the prompt itself but also to the logic written into the code.

In the `files` example discussed in lesson S01E02 — Context, we created a somewhat impressive dynamic memory-building mechanism. However, just a few queries are enough to realize that it lacks many functionalities, and the tool's usefulness is quite limited. Therefore, in this lesson, we will push the boundaries of what is possible by discussing the `memory` example, which only seemingly performs a similar task to the `files` example.

In the lesson S00E04 — Programming, I wrote that prompts in application code almost always consist of several dynamic sections, making them difficult to view and debug. That is why we also went through the LangFuse configuration, which we will now revisit. Additionally, we will find practical use for PromptFoo (or an alternative tool for prompt evaluation).

A lot of theory can be written about Prompt Engineering. Techniques such as Few-Shot, Chain of Thought, or Tree of Thoughts are worth knowing from this perspective as well, but at some point, we will inevitably face the necessity of practical application of this knowledge. The techniques alone are not everything, as the most work will still be careful description of instructions and providing quality examples. Some time ago, all this work would have had to be done directly by us, but from now on, I want a large language model to accompany us in practically **every situation**, and our role will shift more towards that of an architect.

What I am writing about now is justified in publications such as [Large Language Models as Optimizers](https://arxiv.org/abs/2309.03409) and [Large Language Models Are Human-Level Prompt Engineers](https://arxiv.org/abs/2309.03409), as well as from my experience. Although it is currently difficult to talk about a simple query to the model (Completion) being able to generate a flawless result, even with o1 models, they are undoubtedly able to effectively support us. This can occur either through direct interaction or through partially autonomous tools, which we will still be building.

Below is an excerpt from the aforementioned publication, presenting the concept of Meta Prompt, with the help of which we can optimize other prompts.

![](https://cloud.overment.com/2024-09-14/aidevs3_llm_optimizers-7a40ff51-b.png)

Similarly, we can debug existing prompts, generate Few-Shot examples, or test sets for evaluation.

However, all this does not replace our own mind, which is still necessary for setting directions, determining paths, or imposing constraints. Therefore, let's start with the general concept presented in the `memory` example and how it essentially works.

<div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1009451739?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="01_04_memory"></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

The logic of the `memory` example is as follows:

1. Based on the content of the conversation, the assistant asks itself a series of questions that are used to search the memory. It is important that **these queries are not generated solely based on the last message**, which allows for maintaining context. For example, if the conversation is about "Large Language Models," the assistant will repeatedly load information about them. This creates an optimization space, for example by introducing temporary memory, so that the same memories do not have to be loaded each time.
2. Then, with the help of the Vector Store, memories matching the above-generated queries are found. At this stage, they are also filtered, but eventually, we could analyze them with the help of a model to ensure that those entering the main context are actually relevant at that moment of the conversation.
3. After loading the memories, the assistant decides whether it should learn something. All available information is considered here, and above all, **the user's command**, who must clearly emphasize the desire to save data. By default, a mistake can occur here due to the fact that the responsibility lies with the model. However, it is possible to modify the application logic so that the user could, for example, confirm the saving of a memory using an interface.
4. In case there is a need to remember information, the assistant considers the memories loaded in point `2` and decides to add/update/delete data. It is worth noting here that the effectiveness of organization will depend on the effectiveness of the information found.
5. And finally, the collected data goes to the main prompt, which generates the response passed to the user.

The entire interaction scheme is as follows (the image below can be opened in a new tab for better readability).

![](https://cloud.overment.com/2024-09-14/aidevs3_memory_map-f6f865e6-0.png)

## A Few Words About Embedding and Vector Store

If you are already working with vector databases, you can skip this paragraph.

So far, in examples, the topic of vector store or vector databases and embedding has appeared. Although we will talk about it later, we will now make a small introduction. The topic of vector databases can be understood much better when we simply call them **search engines**, similar to ElasticSearch or Algolia. However, here instead of searching for matching ways of writing, we look for phrases similar in terms of meaning. For example, a similar writing style is `king`, `queen`, and a similar meaning is `king` and `man`, as well as `queen` and `woman`.

Word meanings are described using models that generate so-called embedding, which is a set of numbers (vectors) representing various features of the content. In our examples, we use the `text-embedding-3-large` model from OpenAI, but even now there are much better models listed on the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard). Depending on the application (e.g., working with Polish language), other models may work better. Moreover, for new words (e.g., Tech•sistence project), the model may struggle to describe their meaning, and this should be remembered and hybrid search should be used here. 

![](https://cloud.overment.com/2024-09-14/aidevs3_meaning-8c93c129-0.png)

Anyway, so that searching with the help of embedding is possible, entries from our database must first be converted to embedding and stored in a vector database or classic database adapted to handle such a data format.

Subsequently, to search for information, we must convert the search into embedding. Only then can we compare its meaning with the data stored in the vector database and retrieve those with the closest semantic similarity. This mechanism is clearly visible below. The query "Play Music" was converted into embedding `[0.2, ...]`, and the closest match to it is the information about Spotify (`[0.3, ...]`). 

![](https://cloud.overment.com/2024-09-15/aidevs3_vector-c536e191-2.png)

Carrying this over to the `memory` example, from the LLM conversation content, queries are generated, which we convert into embedding, and then search the "`faiss`" vector store. Inside it, we store only file identifiers (`uuid`), thanks to which we can load the actual content of the files.

## Generating Prompts

In the `memory` example, the `prompts.ts` file contains several complex prompts. At this stage, each of them can be considered a draft or simply the first version capable of fulfilling the initial assumptions with the help of the GPT-4o model. Prompts were written with the help of LLM and a special meta prompt similar to the one I provided in lesson S00E02 — Prompt Engineering. Now, let's take a closer look at this.

To begin with, it should be noted that currently large language models already have knowledge about popular prompt design techniques. Moreover, they can use it in practice and combine it with a rich vocabulary and extensive knowledge on various topics. Some time ago, there was even a leak of one of the prompts used by Apple, which contained the phrase "don't hallucinate." It was widely commented on, mostly negatively, although there were also statements that there [may be some sense](https://x.com/benhylak/status/1820894401834741912) in it.

![Large language models already have knowledge about prompt design, e.g., CoT](https://cloud.overment.com/2024-09-14/aidevs3_cot-5887cbb9-4.png)

Returning to the meta prompt, you can download it [here](https://cloud.overment.com/AI_devs-3-Prompt-Engineer-1726336422-1726339115.md). Since it is tailored to my needs, we will now discuss its individual fragments.

In the first part, we standardly assign a role (Prompt Engineer) and narrow the context by mentioning "Large Language Models," ensuring there's no ambiguity on the topic. Right after that, there is an `<objective>` section specifying the main purpose of the prompt. Its placement is not accidental, as LLMs tend to pay more attention to instructions at the beginning and end of a prompt, as described in [Lost in The Middle](https://arxiv.org/pdf/2307.03172). Although some time has passed since the publication of this document, and models now exhibit much better context processing skills, the problem still occurs, as we can also read in [A Challenge to Long-Context LLMs and RAG Systems](https://arxiv.org/pdf/2407.01370).

After determining the goal, we have a rules section, i.e., `<rules>`, containing precise guidelines regarding the way of generating content and conducting interaction. Currently, not much is known about the effectiveness of writing certain keywords in CAPITAL LETTERS, but they can be considered important from the perspective of the readability of instructions.

Among the rules, terms like Chain of Thought or SMART principles appear, aimed at activating model areas associated precisely with these concepts, which we discussed when talking about the `Latent Space` topic.

![](https://cloud.overment.com/2024-09-14/aidevs3_meta_1-b4834a43-7.png)
The next fragment of the prompt is a series of steps or process elements of prompt shaping, which I wrote based on my own experiences. Their presence in the meta prompt is so that the model guides me through it every time we work on a new instruction. Of course, some of these steps can be omitted during the conversation, but it is generally worth stopping at them for a while to ensure that all essential points of the new prompt are clearly emphasized.

![](https://cloud.overment.com/2024-09-14/aidevs3_meta_2-035a2fca-d.png)
In the following section, we have a set of keywords and techniques aimed at directing the model's attention to chosen areas of knowledge, such as the application of popular mental models or avoiding cognitive biases. Including them within the meta-prompt increases the chance that they will be utilized during a conversation about shaping or optimizing new instructions.

![](https://cloud.overment.com/2024-09-14/aidevs3_meta_3-acf500bc-2.png)

Finally, we also have a general prompt template that includes various sections and outlines the way examples are presented Few-Shot. The beginning and end of this section are noteworthy because I used a **different separator in the form of ### as well as capital letters**. The reason is that I could no longer use tags like `<objective>`, as I am already using them in the meta-prompt itself and in the presented template. A different separator allows for **differentiating** the content.

![](https://cloud.overment.com/2024-09-14/aidevs3_meta_4-3c82b7ad-c.png)

Once again, a link to the above prompt can be found [here](https://cloud.overment.com/AI_devs-3-Prompt-Engineer-1726336422-1726339115.md). Using it involves simply setting it as a system message and then outlining the goal of the new prompt we want to create. The model will then guide us through the entire process, step by step.

The above meta-prompt plays an extremely important role, and it's not about generating new prompts faster but about **carefully iterating with the model on subsequent versions of the instructions** and **noticing details and dependencies** that are often hard for us to see. It is also said that large language models are not creative and only reproduce content from training data. However, it is enough to build a few prompts with them together to realize that they can surprise us, and I think we will be observing this soon.

**IMPORTANT:** The goal of the meta-prompt **is not** to generate the prompt for us but to do it together with us. Most of the work still lies on our side.

## Debugging the Prompt

When you read these words, the `memory` example is at the stage where it fulfills basic assumptions related to information organization. So mechanically, everything looks fine, but I still do not know how it will perform in practice. However, a few links were enough to see that the prompt responsible for their organization does not work according to my expectations and incorrectly places memories within the following structure (source: `/memory/prompts.ts`)

![](https://cloud.overment.com/2024-09-15/aidevs3_mind-be1e1739-5.png)

The request for saving the link to the project [heyalice.app](https://heyalice.app) ended with the creation of a note in the `resources -> documents` category, not `resources -> websites`.

![](https://cloud.overment.com/2024-09-15/aidevs3_categorization-40ba59c4-5.png)

Moreover, the LangFuse logs clearly suggest that at the memory-saving stage, the generated user message **indicates the correct category**! Despite this, the model decided otherwise, which is not understandable.

![](https://cloud.overment.com/2024-09-15/aidevs3_request-b5a92d3e-d.png)

So I used the OpenAI Playground equivalent available in LangFuse to send another message marked as "System Check" asking for an explanation of the decision. In response, I received an explanation in which the model considered the `notepad` category more appropriate for the indicated resource. Of course, given how LLM works, we're not talking about 100% certainty that the reason given by the model is what we're looking for, but we can consider it a clue.

![](https://cloud.overment.com/2024-09-15/aidevs3_ask-cce30cad-3.png)

Now we have several options to consider:

- Ask probing questions that may lead us to further prompt issues or new ideas for its further iteration.
- Analyze the prompt independently, especially in terms of missing data, ambiguous instructions, or even the order of individual sections.
- Break the task into smaller steps, considering, for example, thinking about the organization of memories beforehand.
- Allow more time for "thinking" by generating the "thinking" property, placed at the first position in the JSON object structure.
- Introduce changes together with the model.

Let's see how the model can help us with prompt optimization.

## Optimization with the Help of the Model

We know that the model implicitly follows instructions contained in the prompt and user messages. Despite this, it is possible to discuss the prompt by informing the model of this fact beforehand. This can take the form of a message like the one shown below or a situation where we first exchange several initial pieces of information with the model and only then proceed to the prompt.

![](https://cloud.overment.com/2024-09-15/aidevs3_work-2f9301b7-5.png)

Once the prompt is in the conversation and the model reacts correctly to it, we can proceed to introduce changes by describing the issue and the principles for further cooperation. Below you can see how I present the problem and suggest changes concerning the reasoning process and structure of statements, asking for the addition of the `_thinking` property.

![](https://cloud.overment.com/2024-09-15/aidevs3_refining-07aa5f98-1.png)

In this way, the model generated another version of the prompt for me. However, the optimization process is iterative, and one message was not enough to solve all the problems. I reviewed each of the following suggestions from start to finish, asking for further corrections **or introducing them myself**. Then I tested the prompt and checked the application's behavior.

We are talking about a relatively simple mechanism that other users do not utilize. So, we have no problem making even very significant changes to it, but we will not always have such comfort.

Further debugging of the application led me to conclude that it is necessary to modify the memory structure and supplement descriptions so that it is challenging to confuse them. Even the mentioned difference between `resources -> documents` and `resources -> websites` was not obvious to the model. Therefore, I changed the name `documents` to `files` and, together with the model o1-preview, we went through the new version of the memory structure multiple times.

![](https://cloud.overment.com/2024-09-15/aidevs3_ambiguity-aa9030ed-4.png)

There is no doubt that the language model is not able to solve problems related to prompt operation independently, especially when there is no reference point in the form of test datasets and feedback from the application. However, the value of the model's skills in cooperation with humans is undeniable.

## Prompt Compression

The same sentence can be expressed in different ways, using more or fewer words. In the case of prompt, this is important concerning tokens and managing the model's attention. Similarly to creating a prompt or analyzing its operation, we can use the skills of the language model in paraphrasing and compressing instructions. More on this topic can be read in [LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models](https://arxiv.org/abs/2310.05736) or by simply using the tool discussed in this publication [LLMLingua](https://github.com/microsoft/LLMLingua).

The repository of this project contains the following graphic, illustrating the process of compression, which largely involves removing words that do not matter from the content point of view.

![](https://cloud.overment.com/2024-09-15/aidevs3_compression-5eb423a4-d.png)

My experience with this tool suggests that **automatic prompt compression provides moderate results**, but it is a good source of information about what we can potentially remove. Here, we also have space for creating meta-prompts capable of effective compression.

Prompt compression seems to have moderate significance considering the increasing limits of context windows or prompt caching. However, we already see that complex models have problems following complex instructions, which also **quickly become incomprehensible for humans**.

In terms of compression, we can consider: 

- Replacing extensive, complex prompts with smaller, more specialized actions that will be conditionally activated, depending on the situation.
- Describing selected model behaviors with the help of single words or phrases, not full sentences. For example, the term "U**se first-principles thinking**" indicates a concept of reasoning understandable to the language model by breaking down the problem into fundamental components.
- Changing the prompt's language and context, e.g., from Polish to English.
- Removing selected parts of the prompt that describe the model's natural behavior (e.g., the way statements are formatted) and therefore do not add anything new. Moreover, such instructions could **limit the model**, reducing its effectiveness.

As with optimizing the prompt for effectiveness, we can discuss the topic of compression with the model. I had the following conversation with the o1-preview model, which provided me with a comprehensive report and a suggestion for a new version of the prompt in the first round.

![](https://cloud.overment.com/2024-09-15/aidevs3_optimize-22e0c2ad-b.png)

![](https://cloud.overment.com/2024-09-15/aidevs3_optimization-0c4963b9-0.png)

Most of the suggested changes were very accurate and drew attention to overly general instructions or fragments concerning the same behaviors. Among the suggestions, there were also some that did not align with my assumptions, so I simply omitted them. The work result is visible below — a reduction from 2059 to 803 tokens while maintaining expected effectiveness.

![](https://cloud.overment.com/2024-09-15/aidevs3_uncompressed-216b3e0d-4.png)

![](https://cloud.overment.com/2024-09-15/aidevs3_compressed-77fdaf30-a.png)

The prompt compression process will vary depending on the situation and will not be one-time. During subsequent modifications, we will quickly reach a point where the prompt again increases in volume and requires improvement.

## Model Response Optimization

The cost of the number of generated tokens is greater than the cost of tokens sent to the model. Of course, there are almost always many more of the latter, but even so, the output tokens have more impact on prices and application operating speed.

![](https://cloud.overment.com/2024-09-15/aidevs3_pricing-bc448bed-9.png)

Here is the `input -> output` token distribution for the last few queries of my system. The token response accounts for ~5% of all processed tokens. Considering the speed of inference, which we discussed in the lesson S01E03 — Limits, the desire to **achieve a possibly short and/or precise model response.**

![](https://cloud.overment.com/2024-09-15/aidevs3_inputoutput-bb3f2e00-1.png)

A good example is the `_thinking` property, which appears first in my prompts as the first property of the generated JSON object. In this way, I provide the model with space to reflect on what the subsequent properties of the returned object should look like.
If our instruction in this situation suggests the model to "ponder" exclusively, we are likely to receive an extensive content of questionable value. However, when we indicate a thought pattern or a list of questions that the model must answer, the quality of the response significantly increases.

![](https://cloud.overment.com/2024-09-15/aidevs3_output-a0be0809-b.png)

The above instruction describing the pondering process yields excellent results because the model lists keywords whose presence contributes to higher quality generated queries in the `q` (queries) array.

![](https://cloud.overment.com/2024-09-15/aidevs3_reason-f7daba9b-8.png)

The style of expression is not only defined in the template but also applied in Few-Shot examples. This ensures the model actually follows our instruction, especially at the beginning of its responses. In longer conversations, a tendency to revert to the model's "natural" style is often noticeable.

One solution to this problem is keeping instructions as short as possible along with the number of conversation messages, which can be stored only as a summary, as we discussed in lesson S00E04 — Programming
## Fine-tuning

Model behavior can be tailored not only through instructions or provided context but also through the fine-tuning process, which allows us to specialize the model in a specific task. Fine-tuning should not be viewed as an alternative to model control through prompts but as a supplement to this process.

The following example of "optimization techniques" comes from the recording [A Survey of Techniques for Maximizing LLM Performance](https://www.youtube.com/watch?v=ahnGLM-RC1Y) shared by OpenAI. It shows two types of optimizations — **behavioral** and **knowledge**.

![](https://cloud.overment.com/2024-11-01/aidevs3_optimize-a486ea6d-5.png)

Thus, the application logic can consist of a series of prompts implemented by a model like GPT-4o, but individual stages can be handled by a model specialized in this task.

In the case of OpenAI, the fine-tuning process can currently be performed directly on the [platform.openai.com](https://platform.openai.com/finetune) panel and is not complicated. The difficulty of this task lies in selecting the appropriate training and testing data sets, and subsequently the evaluation of the model. Often, we will generate this data with the help of the model (for which prompts will be needed beforehand) or process data directly from our application (while maintaining privacy policy).

Through the basic fine-tuning process, which you can undertake now, Jakub will guide you in the following video:

<div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1025471081?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="fine-tuning-gpt4o-mini"></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

## Summary

To summarize today's lesson, when designing applications utilizing large language models, the models themselves prove particularly helpful. However, their current capabilities do not allow for fully autonomous operation without any human oversight. Instead, we can design tools specialized in specific areas. By tools, I mean **prompts, series of prompts, or partially autonomous systems combining application logic with models**.

After this lesson, there is likely no doubt about the crucial role instructions play for the model. At the same time, it is clear that this is not only about the prompts themselves but also how they are organized in code, as well as the code itself. Because even looking at the `memory` example, we see how important it is to control the flow of data between prompts, monitor them, and conduct automatic tests.

If you are to do only one thing from this lesson, familiarize yourself with the mentioned meta prompt and simply start using it, then gradually modify it to suit your needs. A very good idea is also to send several inquiries to the `memory` example to observe how the model behaves. Knowledge of this will certainly be useful in designing subsequent tools. Therefore, I encourage you to watch the video below and test the memory functionality yourself.

<div style="padding:75% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1009759972?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="01_04_memory_2"></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

Good luck!