![](https://cloud.overment.com/S03E03-1732023347.png)

Lesson S03E02 showed us that vector databases are not sufficient for effective data searching. While model support improves the situation, it doesn't solve all problems. For example, searching for acronyms, series numbers, or orders, or expressions that a model cannot describe in an embedding, might be challenging.

Thus, when dealing with keywords and precise matching, vector databases prove insufficient, and semantic search must be complemented by full-text search.

Combining different search techniques is known as hybrid search. It can take various forms and configurations depending on the needs. For example, sometimes using PostgreSQL with pgvector and pgsearch for storing data and semantic and full-text searching may suffice for small projects. Other times, we may care about separating these responsibilities into different tools.

In this lesson, we will focus on hybrid searches, which will allow us to build advanced Retrieval-Augmented Generation systems, including memory for an AI agent.
## Classic Search Engines

Search engines like Algolia and Elasticsearch enable advanced searching with full-text matching, fuzzy search, narrowing the search area (faceted search), filtering, and evaluating returned results.

The basic interaction is similar to vector databases. We have an index, and in it, documents possess their properties. This can be seen by creating an account on [algolia.com](https://algolia.com), from which you can obtain an application ID and API key (administrator). Then, after launching the `algolia` example, an `dev_comments` index will be created on our account, and three records will appear.

![](https://cloud.overment.com/2024-10-08/aidevs3_index-2614602e-4.png)

The test query returns a match to two of the three documents.

![](https://cloud.overment.com/2024-10-08/aidevs3_algolia-3991529f-f.png)

If we want to search entries from only one author, we first need to specify the fields in the index settings that we want to filter by. In our case, it will be the `author` field.

![](https://cloud.overment.com/2024-10-08/aidevs3_facets-f4fad0d3-f.png)

This time, the search returns only one entry, matched to the author specified in the filter.

![](https://cloud.overment.com/2024-10-08/aidevs3_limited-0a64152f-9.png)

At this point, we already have the basics of working with a classic search engine, and we are practically ready to implement the logic for hybrid searching.
## Data Synchronization

We can already see that documents in Algolia have their identifiers. The same applies to Qdrant and the SQLite database. This means all records can be linked, with the whole system developed so that **the database acts as the source of truth.**

In the `sync` example, I've prepared logic for synchronizing information between three sources: the database, the vector base, and the search engine index. Thus, any changes made in the database will be mirrored in the remaining sources.

![](https://cloud.overment.com/2024-10-09/aidevs3_interface-10b75a3c-c.png)

The Algolia index contains all document properties, including a unique identifier.

![](https://cloud.overment.com/2024-10-09/aidevs3_algolia_doc-f69dd56b-b.png)

It looks similar in Qdrant, where we also have a complete set of information, including the same identifier.

![](https://cloud.overment.com/2024-10-09/aidevs3_qdrant_doc-eb5e613c-3.png)

And finally, we have our source of truth, i.e., the SQLite database that our application operates on.

![](https://cloud.overment.com/2024-10-09/aidevs3_database_document-cf9e1e96-8.png)

Even for applications operating on a slightly larger scale, using three different tools is not difficult and offers great possibilities, as we will soon discover.

Regarding data synchronization:

- We can use only PostgreSQL and available extensions to store and search data in various ways. This approach can be convenient, but limited. Here, you can also consider using [Supabase](https://supabase.com/).
- We can create a **common interface** that automatically synchronizes data between different sources of knowledge. Here, maintaining a **common identifier** is the priority.
- Synchronization will not always involve exactly the same data for all sources. Even in our case, data stored in Qdrant additionally contains embeddings, which are not present elsewhere.
## Selection of Indexed Content

A pertinent question is what to store in the original database versus search engines. The answer will always depend on our individual needs, but we can answer some general questions and **consider model involvement in content transformation.**

So:

- **What data does our interface-side application require** and how will we display it? The answer to this question determines the structure of information stored in the database or at least indicates directions.
- **Which data fragments will be used for searching** and how do we want to filter or limit access to them? The answer to this indicates the content that should go to search engines.
- **What data is missing from the original documents, and how might LLM help in its creation?** This mainly concerns classification, titles, descriptions, summaries, or other transformation forms.
- **Does a large amount of data hamper searching, and if so, what can we do about it?** This supportive question helps identify the need for additional filters and categories to be considered during indexing and recovery/searching.
- **Which data is permanent and which is temporary?** For example, entries in the database might be saved only for an ongoing conversation or even a single task being realized during the conversation. We must ensure easy loading and exclusion from main search results unless it conflicts with the application's logic.

Reflecting on these questions helps outline the shape of a structure that can be translated into all data sources. The quality of these answers will also directly impact the effectiveness of the entire system.
## Hybrid Search

In the `hybrid` example, there's an extension of the `sync` example, but with a different dataset for document searching. The point is, we use two search engines, resulting in **two lists of documents that need to be merged** using Rank Fusion. Specifically, documents receive a ranking based on the sum of the inverse of their position from each list. Here's that code snippet:

![](https://cloud.overment.com/2024-10-09/aidevs3_rrf-7c378bd6-0.png)

Simply put:

- Store both search results as **one list**
- Calculate their `score` using the formula `(1 / vector rank) + (1 / full text rank)`
- Sort by this ranking

Of course, we can change the weights for each list to alter its impact on the ranking, increasing or decreasing the importance of vector or full-text searching.

Merely calculating the ranking isn't everything, as browsing the code primarily reveals the use of **two queries**. One, written in natural language, is directed to Qdrant, while the other contains only keywords and is aimed at Algolia.

![](https://cloud.overment.com/2024-10-09/aidevs3_search-b091fa01-7.png)

The test dataset includes a list of books by Simon Sinek and Jim Collins. Only three directly contain the phrase "people," but most of them indirectly address this topic.

For the query `Find me everything about people`, Qdrant returns titles in which descriptions featured the phrase `people` in positions 1, 2, and 5. Meanwhile, Algolia returns only three records, but the word `people` appears in each one.

![](https://cloud.overment.com/2024-10-09/aidevs3_search_results-8d444b64-7.png)

We cannot unequivocally claim that Algolia's results are superior because, in Qdrant, the third and fourth positions include a book on leadership, which is directly related to our query.

Applying RRF to sort results, we obtain a TOP3 with books containing the phrase `people`, while the mentioned titles on leadership appear right alongside.

![](https://cloud.overment.com/2024-10-09/aidevs3_ranked-d3e6fc8c-3.png)

Undoubtedly, this is the best result achievable through hybrid searching.

There are also cases where search effectiveness isn’t comparable between both strategies. For instance, if a query focuses on acronyms, serial numbers, or typos, full-text search has significantly more value.

Below is an example search using an ISBN (randomly generated values). Here, the vector base returned a document, but only at position 8 out of 9. Meanwhile, Algolia returned just one document. As a result, the sought-after book landed in 1st place.

![](https://cloud.overment.com/2024-10-09/aidevs3_isbn-92f982dc-b.png)

Naturally, there will also be reverse situations where keyword matching will be entirely absent. In such cases, Algolia won't return results, while Qdrant will likely find the right documents.
## Augmenting Queries and Self-query

What if the issue with retrieving desired content doesn’t lie with the search engine, but emerges at the query stage? In such a scenario, we can engage LLM to modify the original query or generate a series of new queries based on it.

A simple interaction exemplifies this, like asking the AI Agent “Hi, what's up?” Such a general query is difficult for both search engines to find potentially relevant documents.

However, if the model considers, for instance, the time of day, current location, or very general information about itself, it can generate several self-directed questions. This way, we obtain content that can already be used for database searching.

![](https://cloud.overment.com/2024-10-09/aidevs3_self-query-3c21e763-0.png)

Looking at the above queries, we see they are **personalized**, resulting from the initial context, which is the agent's personality description and key details. The general rules are also included, such as:

- Ask questions about the mentioned people, projects, or tools
- When the answer may concern you, load your profile, mentioning your name "Alice"
- When a question concerns the user, always mention their name (Adam)
- For general messages, scan the list of available categories for a potential topic to help continue the conversation and make it engaging.

Such rules significantly enrich the interaction with an agent capable of adaptation to different situations. It's worth adding that using keywords like names is intentional, as the generated list is intended for search engines.
## Filtering and Processing Search Results

In the previous lesson, we already saw an example of re-ranking performed by a model evaluating individual results for their relevance to the current interaction. Of course, there's nothing preventing the continued use of this method in conjunction with hybrid search.

However, there may also be situations where we are not interested in the precise retrieval of a few/several documents but rather in retrieving all documents from a given category. In such cases, we will want to engage either solely a database or a search engine like Algolia for the task. Then, the role of the LLM will be to return, for example, identifiers or the names of categories and filters to be applied.

A situation where this may be necessary involves queries like: "list all books by authors born in XXXX" or "summarize the results of reports from the years 2010-2024". The search engine then returns a large number of documents, which also include a lot of content irrelevant to the current task. Instead of loading them into the conversation context, we can use logic that creates a summary focusing on just one topic. Additionally, the result of such a summary can be saved for future interactions to avoid the need to repeat this process.

Thus:

- The LLM can decide which areas of the database to **filter** to retrieve a set of documents matching the filter, rather than a specific query.
- The list of retrieved documents can be directly passed to the context or processed to extract only the selected data precisely.
- The result of the processed documents can be saved in the database as a **new document** to limit the need for executing the same action multiple times.

The implementation of the above points can be based on examples we have already completed (e.g., `summary`).

## Delivering Search Results to the Context

Metadata is useful not only for filters at the search stage but, above all, at the stage of delivering the document content to the context. The main content is usually insufficient for the LLM to use effectively.

The prompt below shows in practice how important such a form of delivering documents is. If we hadn't considered the creation date "created_at", the model would respond to the user's question positively, but this way it refused, truthfully.

![](https://cloud.overment.com/2024-10-10/aidevs3_context-6013fb1b-e.png)

Similarly, we can separate information coming from different tools that the agent uses. However, it is not worth overusing this and we should always aim for only the most important documents to be in the given context.

In formatting the context this way, it turns out that using syntax similar to `xml` is useful, as it clearly separates not only the individual contexts but also fragments of content within them. An example might be a list of search results containing results for more than one query.

![](https://cloud.overment.com/2024-10-10/aidevs3_results-8847e013-6.png)

## Summary

Building Retrieval-Augmented Generation without applying hybrid search should practically not be considered from now on. Especially since configuring full-text search and subsequently evaluating results is quite simple.

At the same time, it should be clear that even the best search systems are not enough to address a possibly large portion of queries and their categories. All this leads to the conclusion that **universal RAG systems** trying to fit any kind of data are still beyond our reach if we care about very high effectiveness.

An enormous role is also played by elements supporting the search process, such as query enrichment (e.g., Self-Querying), or model-based result evaluation.

If you are to take only one thing from this lesson, familiarise yourself with the `hybrid` example and try to run it on your own free Algolia and Qdrant accounts in conjunction with a dataset generated by you (you can use LLM for this).

Good luck!