Large language models enable seamless natural language processing. Combined with Text to Speech and Speech to Text models, we can design advanced voice interfaces that were out of reach just a few years ago. This opens another chapter in the development of voice interfaces and tools capable of processing audio and video recordings.

The market now offers tools providing access to TTS/STT models and complete solutions for specific applications. For example, [Happyscribe](https://www.happyscribe.com/) is an excellent solution for transcription, additionally offering an API. In this case, instead of creating your own solution for processing long audio/video materials from scratch, it's much better to use an existing tool.

On platforms like ChatGPT and the impressive "Voice Mode," the possibilities for personalizing experiences are very limited. In such cases, it may be justified to build your own integration from scratch, utilizing our knowledge of generative AI and programming skills. Now, we also have access to the Realtime API, and models can also accept audio files as part of a user's query. Currently, however, the costs of such interactions are extremely high and limit us to using OpenAI models (although audio files can be accepted by other models, e.g., Gemini).

In this lesson, we will focus on building our own integration, where we combine Text to Speech, Speech to Text, and LLM models. The goal is to create an interface that allows vocal conversation with the model, without the need to use a keyboard or buttons.

## Audio from a Programmer's Perspective

In this lesson, we will discuss the `audio` example, which also consists of an interface available in the `audio-frontend` directory. It contains quite complex logic related to microphone handling, audio playback, animations with HTML canvas, or back-end communication. If you don't work in JavaScript or are unfamiliar with front-end or back-end aspects, focus on the mechanics we will discuss rather than implementation details.

If you wish to explore the examples mentioned, and audio processing is new to you, use the assistance of models (especially Sonnet 3.5 and o1) and browser API documentation on [MDN](https://developer.mozilla.org).

## Current TTS, STT, and Generative Audio Capabilities

Text to Speech and Speech to Text models are developing quite rapidly, but similar concepts underlie them as language models, which also translates to similar limitations (e.g., confabulations, limited controllability, or token window limits), which we will confirm in a moment.

Building an audio interface consists of three main components:

- **Input**: Sound source - microphone or audio file
- **Logic**: Processing - transcription and generating responses
- **Output**: Final effect - streamed audio or sound file

So we will usually need three models here:

- Speech to Text: e.g., Whisper, Deepgram, or AssemblyAI (supports speaker recognition, i.e., diarization)
- Text to Text: i.e., large language models from OpenAI, Anthropic, or others
- Text to Speech: e.g., OpenAI TTS, ElevenLabs, or [Neets.ai](https://neets.ai/)

Already, the effectiveness of generated transcriptions is high but not perfect. You can experience this first-hand thanks to the Groq platform, where we have access to Whisper Large v3. We see there immediately that Whisper supports formats such as flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, and webm, and the file size is limited to 25MB. This means that for longer files, compression or division into smaller fragments will be necessary.

Transcription often needs to consider proper nouns or keywords specific to the content being processed. Therefore, to increase the likelihood of correct transcription, we can use a system prompt containing our own dictionary. Unlike LLM, we are not talking here about instructions the model must follow but rather a piece of content that simply precedes the transcription content. Often, in automation cases, the system prompt content is set to transcription from the previous audio fragment. Here, the token limit is limited to 224 (encoded using [Multilingual Whisper tokenizer](https://github.com/openai/whisper/blob/main/whisper/tokenizer.py#L361)). More about prompts for the Whisper model can be read in the [OpenAI Cookbook: Whisper Prompting Guide](https://cookbook.openai.com/examples/whisper_prompting_guide).

Whisper also has a very distinct problem with processing silence and very short text fragments (e.g., commands). For the following 5-second silence recording, the returned transcription contains the text "Thank you." Sometimes it can be text in another language or terms characteristic of a YouTube video's ending (e.g., thanks for watching). This means we should avoid processing audio fragments that do not contain speech.

It is also worth knowing that the Whisper model can successfully run on your computer, and thanks to projects like [Insanely Fast Whisper](https://github.com/Vaibhavs10/insanely-fast-whisper), it can prove to be a very effective solution.

Meanwhile, regarding Text to Speech models, over 2023-2024, we observed their significant development, not only in terms of speech quality but also **voice cloning** and simulation of emotions, as well as sounds such as laughter, breath, or even singing (an example here is [suno.ai](https://www.suno.ai/)).

In our case, we will usually convert text to voice, primarily for the needs of new content (e.g., bot responses or a private podcast). The reason is that processing existing content, such as translating videos, remains a significant challenge due to the need for audio synchronization with the original track. The second challenge concerns the ability to control the way content is spoken. For example, the influence on intonation or the way specific phrases are pronounced is almost zero.

It is also worth adding that Speech to Speech (or voice to voice) models, such as [Hume.ai](https://www.hume.ai/), are emerging on the market, capable of recognizing the speaker's emotions. This means that input data significantly impact the generated audio. On the other hand, it is still necessary to include logic allowing the model to connect with external tools and knowledge sources. In such a situation, we will depend on processing the audio so that we can take all required steps before generating a response.

We will soon proceed to implement this logic in practice, but it should already be clear how we can utilize the discussed examples, such as `memory` or `linear` with the audio interface.

## Capturing Sound Sources and Audio Formats

In the `audio-frontend` example, the `app.ts` file contains the `startRecording` function, which is responsible for connecting to the microphone and analyzing the sound track for visualization, silence detection, and transcription purposes. With such data available, we can build a buffer with the audio recording and gradually send it to OpenAI or Groq for transcription. Meanwhile, when the user stops speaking, we send the message content to chat with the large language model.

Therefore, the general scheme looks as follows:

1. **Microphone:** Connect to the microphone and attach **monitoring** of the sound track
2. **Silence Detection:** Based on the current audio level, detect whether the user is speaking at that moment. Once silence is broken, recording begins.
3. **Transcription:** Transcription is updated every 1 second based on the recording so far.
4. **Response:** When there is silence for at least 1 second, recording is paused, and assistant response generation begins, followed by its playback
5. **Next Round:** After the assistant's speech, the recording buffer is reset, and the next round of user speech begins
6. **Loop:** The process is repeated until the connection ends

In this case, we use the `.wav` format, allowing us to segment the recording into fragments and process them independently. Alternatively, we could use the `.ogg` format, which, thanks to compression, would allow us to reduce the size of transmitted data, although I opted to reduce the sampling frequency to 16,000 Hz mono.

In the `audio-frontend` example, I avoid using external libraries, but it is advisable in production due to browser support or easier detection of silence in the recording. However, the choice of tools will vary depending on the project and technology we are working with.

## Generating Transcriptions and Speaker Recognition

I mentioned that tools like [Happyscribe](https://www.happyscribe.com/) or AssemblyAI are much better for transcribing longer content formats (e.g., podcasts, movies, or meeting recordings). Although in some cases, using the Whisper model or a similar alternative may be advisable (e.g., due to costs), even the interface below will usually suffice to opt for ready-made solutions.

The availability of the API of both mentioned tools further justifies their choice, as we can still benefit from partially automating the process.

If you decide to work with the Whisper model, the most attention will require dividing the recording into segments, as seen in the `audio` and `audio-frontend` examples, so we won't discuss this further.

## User Interface and Real-Time Interaction

Examples `audio` and `audio-frontend` **must be run simultaneously** to gain access to a visual interface enabling real-time conversation with LLM. The application server will thus operate at `localhost:3000`, while the front-end will be available in the browser at `http://localhost:5173`. Let's see how this whole mechanism works in the video below.

By visualizing the `audio` example's logic, we see no particularly surprising elements, but we must remember a few details affecting interaction quality.

Here we are talking about:

1. **Audio Quality:** Enabling the user to verify the audio recording quality beforehand to avoid generating incorrect transcription.
2. **Silence:** Dynamic detection of silence levels based on which the system will recognize when the user stops speaking. Potentially, this mechanism can also be used to interrupt the assistant's speech.  
3. **Context:** The quality of transcription depends not only on the audio recording but also on the given context, with particular attention to user-specific keywords (e.g., name).  
4. **Confirmation:** In the case of an interface capable of using tools, it's worth considering the necessity of confirming action execution, e.g., with buttons or voice.  
5. **Correction:** The generated transcription can be further processed by a language model for formatting and error correction.  

![](https://cloud.overment.com/2024-09-23/aidevs3_voice-aab7724c-0.png)

Looking at the voice interface, we can also consider everything we've learned so far about interaction with LLM, as well as integrating it with external tools or long-term memory.

Moreover, there is nothing to prevent voice interaction from taking place not with a human, but between two AI agents, similar to the case with [NotebookLM](https://notebooklm.google/), where we can generate a conversation about our documents.  
## Audio Generation Optimization Techniques  

In the `audio` example, it's possible to reduce reaction time by switching transcription to the Groq service and generating the assistant's response via ElevenLabs with the "turbo" model. However, this isn't the end of optimizations, as we can further work on changing the `wav` format to `ogg` and also consider streaming.

Specifically, the assistant's response can be returned in segments and sent to ElevenLabs in this way, which also allows streaming the generated recording. This means that:

- The first segment of the assistant's speech (e.g., the first sentence or up to the first comma) is sufficient to start audio generation.
- The audio response itself can also be queued for playback.

In this way, we can reduce reaction time, achieving results even below one second. However, in practice, the saved time will be needed for logic responsible for context building or decision-making about tool selection. Maintaining real-time interaction becomes a significant challenge and requires applying content generation optimization techniques we've already discussed. Specifically, I mean:

- Implement streaming where possible
- Execute actions in parallel that are not dependent on each other
- Use smaller models and/or platforms offering fast inference
- Shorten the model's speech
- Use platforms like [Hume.ai](https://hume.ai/) or [Bland.ai](https://www.bland.ai/) when we aim to build interfaces enabling conversations with LLM  
## Summary  

It is well known that the usefulness of voice interfaces in recent years has been very low or allowed only very simple interactions with devices. Now, even in the `audio` example, we see that quite a lot is changing in this area, but it's still hard to talk about 100% effectiveness.

Although real-time interactions will only work in selected scenarios, the ability to process audio recordings may prove useful both in the context of simple commands and automation operating "in the background."

From a programming perspective, it is noticeable that Web APIs are gaining importance, enabling interaction with microphones and audio and video playback, as well as file transfer between the client and server.

If there's one thing to take away from this lesson, it's to familiarize yourself with the `audio` example and try to replicate your own interface that allows conversation with a language model, bypassing the mechanisms related to silence detection or audio streaming. LLM and the Cursor editor can assist you in building such an interface.