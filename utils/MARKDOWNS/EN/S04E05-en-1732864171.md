![](https://cloud.overment.com/S04E05-1732795281.png)

Simple tasks performed by LLM can be simplified to a single HTTP request. If it fails, just repeat the query. However, in practice, we also need complex tasks consisting of multiple steps. Additionally, regardless of the complexity level, we want to keep a history of the actions taken.

We need logic that allows for task planning, scheduling, and resumption of those that were not completed correctly without starting them from scratch.

The logic of queuing queries and processing extensive background tasks is not a new concept in programming. Therefore, we can apply an [event-driven design](https://en.wikipedia.org/wiki/Event-driven_architecture) approach considering real-time communication, which is also suggested by real-time API. It is worth considering this direction due to the ability to easily report progress and fully asynchronous communication. Initially, however, it suffices to have the ability to create a list of tasks and associated actions and documents.

In this lesson, we will build a database structure and interface that connects our previous experiences in creating tools for LLM with external data integration. We will focus on mechanics and key concepts, and the practical use by LLM will be discussed in the last week of AI_devs.
## Data Structure

For task management, we will need a database structure that considers:

- A history of conversations assigned to the user
- A list of messages related to the conversation and documents
- Tasks associated with the conversation and action list
- Actions associated with tools and documents

In other words, we want a structure that allows recording interactions with the model while also considering the actions taken. However, it's not about monitoring the model's behavior, as we did with LangFuse, but about saving data relevant to controlling the agent's (or agents') logic.

A sample schema looks like this:

![Image description here](https://cloud.overment.com/2024-10-23/aidevs3_schema-06ab2daf-d.png)

Of course, details about the types of tables, columns, and connections between them will change depending on the application we create. However, the above scheme can be considered a good starting point.

When launching the `events` example, a SQLite database will be created in its directory, and a simple simulation of a conversation between a user and an assistant will be executed, where there is a need to transform a simple document. It consists of:

- **Document creation**: this step can be compared to when a user uploads a file to a server. Document information is saved in the database.
- **Tool creation**: in practice, the list of tools will already be available, so we won't create them. The agent will simply decide which tools to use for particular actions.
- **Create / retrieve conversation**: based on the `conversation_uuid`, retrieve or create a thread with a list of messages.
- **User message**: the user sends a request for "report analysis."
- **Task creation**: a new task is added to the conversation.
- **Action creation**: the action is assigned to the task and document.
- **Status updates**: statuses of actions and tasks are updated as 'completed.'
- **Response**: the user receives a completed document.

![Image description here](https://cloud.overment.com/2024-10-23/aidevs3_processing-eaa9b7e3-a.png)

Such logic allows us to conduct very flexible interactions, including creating multiple tasks and associated actions. Nothing prevents the system from cyclically receiving "commands" or even sending them itself.
## Model Responsibility and Programming Support

The model's role in logic should be limited only to tasks that cannot be performed programmatically. All other activities should remain in code. For example:

- **Default properties**: default JSON object properties should be set automatically (e.g., date range for search).
- **Verification**: When classifying to a specified category list, we must always check programmatically if the returned response is one of the permissible names.
- **Reference**: If the model can point to a needed resource, it's better to choose that option rather than forcing it to rewrite content.
- **Loops**: If the logic allows the model the freedom of self-reference, we must impose code restrictions to prevent looping.
- **Context**: While the model should be able to indicate documents to load in context, the programming layer should define how they are presented and the scope of considered properties.

In other words, we must always ask ourselves whether the model has to be responsible for a given action and minimize its involvement wherever possible.
## Effective Use of Tools

We have already learned about effective tool use during their construction. Now, however, we see a logic capable of using them independently. Combining this with our experience in both programming and working with models, we can connect facts and notice that:

- **Instruction control**: User instructions are usually very imprecise, and a model using tools requires that precision. We can "enrich the user's query" or ask them for details, or **limit contact with the model to a minimum**. No one said the queries to the model must come from humans; they can be generated as a result of **actions the user takes**. For example, adding a file sends a query like "Load the content of document `[[uuid]]`, list all issues from category `...` and send it to email `@`."
- **Compression**: Dividing into `task > action > tool` and `action > documents` provides flexible access to data and control over their detail. We can describe a task using only **task, action, and category names** or extend it with **descriptions, context, and connections**. This approach supports **managing the amount of information directed to the system prompt**, which promotes "noise reduction" in the form of unnecessary content.
- **Resumption**: Both `task` and `action` have statuses that allow reacting to problems and continuation in the place of the error. The reaction can be automatic, where the model itself decides on input data correction. However, we must always implement programming limits, such as a maximum number of attempts before marking the task as `failed`.
- **Cache**: Some actions will be very repetitive, and there may be no need to perform them multiple times. Queries, however, will not be **exactly the same**, and we won't be able to compare them. Therefore, it's worth considering a so-called `semantic cache`, which indexes **user queries** and searches them to find the most similar ones.
- **Division**: Dividing tasks into `actions` allows distinguishing small, singular activities on which the model can focus at a given moment. This detail significantly affects the whole system's effectiveness - the model's distraction risk is lower with simple tasks than with complex operations on large data sets. Example: generating a nested JSON object structure is much more challenging than generating several smaller objects and programmatically combining them into a whole.
- **Verification**: At both the `task` and `action` stages, we can add prompts that verify whether the model's response is correct, as we know that "verification is simpler than generating."

Furthermore, in the case of prompts requiring long-term memory or a large set of external data, it is worth considering prompt caching and involving the Gemini 1.5 Pro model (Google DeepMind), characterized by a very large context window limit.
## Waiting for a Response

In terms of tasks (`task`), one property specifies whether they should be executed asynchronously. This means that the user will only get confirmation that the task has started. The result will be delivered to them via email or another communication channel. In such a case, it's worth considering using `web sockets` or alternative solutions for real-time applications.

In user experience (UX) design, it is commonly said that **the user has no problem with waiting, as long as they see progress**. An example could be going through a multi-step registration process with a visible progress bar. Another situation may involve using a search engine with extensive filters, where each subsequent step brings the user closer to their goal.

Therefore, when building generative applications that perform complex tasks, it is worth considering an interface displaying current progress. It can also include the option of contacting the user at any time, including interrupting the operation.

An example here could be [real-time console](https://github.com/openai/openai-realtime-console), which is a repository showcasing the capabilities of the [Real-time API](https://openai.com/index/introducing-the-realtime-api/) from OpenAI. Currently, using this service is expensive, but the concepts it contains can be utilized in conjunction with any models.

![Image description here](https://cloud.overment.com/2024-10-23/aidevs3_realtime-2c6e76c8-f.png)

If you already have access to the OpenAI Real Time API, try launching the mentioned console. However, remember that **the cost of using it at the time of writing these words is very high, so familiarize yourself with the current pricing first**.
## Query Queuing

Practically every API we will work with has a rate limit that can effectively interrupt our task execution. In the case of language models, limits include **number of queries**, **number of tokens per minute/day**, **number of input tokens**, **number of output tokens**, and also the budget. Additionally, for services like Anthropic, we must also consider frequent API availability issues.

The simplest way to bypass limits is to use libraries like Vercel AI SDK or frameworks like LangChain. We can also easily make our own integration to control the number of queries and wait for the service's availability again.

In the case of OpenAI and Anthropic, we also get information about current limits, which are stored as HTTP headers. Based on their values, we can determine the waiting time for further queries.

![Image description here](https://cloud.overment.com/2024-10-23/aidevs3_rate_limit-a9b3550c-3.png)

Rate Limit also applies to other services with which we will integrate. An example is FireCrawl, which has significant constraints related to the number of concurrently crawled pages.
## Specialization and Generalization

Up until now, programming has been characterized by deterministic solutions. When designing them, we aim to describe all possible scenarios in the code and lead data flow to achieve the desired effect. Furthermore, **this is also the expectation of our employers, clients, and users**. However, we already know that in the case of generative applications, such specificity is not feasible, and we have transitioned from "certainty" to an area of "probability."

So the question is: **what can we do about it?**

The current generation of language models is advanced enough that the term itself no longer fully expresses their capabilities. Despite this, we personally experience their current limitations, including the challenge of guiding them even towards a chosen target.
In other words â€” **the time when language models will be able to autonomously perform very complex tasks has not yet arrived**.

For example, if we create a **project management system** integrated with an extensive solution like ClickUp, it's difficult to design it flexibly to fit any organization. However, it is within our reach to adapt the system to ourselves, our team, or the entire company. This is because everyone uses tools like ClickUp slightly differently, and a lack of context in this regard will reduce the model's effectiveness.

So, we are talking here about **very narrow specialization**.

On the other hand, based on the example of `todo`, we have learned that even such a system **will not operate** based on **strictly defined rules**, but rather **general principles describing the selection of actions and how they are executed**.

Thus, we have here **a combination** of generalization and specialization, the boundary of which we will push further towards autonomy, with the development of models and the ecosystem of tools.

The conclusion is as follows: **when designing generative applications, we cannot follow the patterns we know from classic applications**. The reason is that we have a completely different category of tools at our disposal, and we play by new rules.

## User Engagement

We have already talked quite a lot about **verification**, **feedback** from the user, and **limiting contact**. However, there are still a few issues:

- **Personalization:** a well-designed system should be capable of **adapting to the user**. This refers not only to dynamic prompt fragments like names, project titles, or building long-term memory but also to **drawing conclusions from interactions and autonomously improving prompts**.
- **Transparency:** there is much discussion about issues related to the interception of the system prompt by the user. However, in practice, their **sharing** can be beneficial, as it helps to better understand the system's operation, even for non-technical people.
- **Redirection:** whenever the model generates a new document, a CRM entry, or creates any new resource, it is advisable to generate a link directing to it. This is a good practice, as it also allows for quick verification of the model's response. In most applications, we can generate such a link using the parameter `?id=7bbe3` or its equivalent.
- **Confirmation:** we know that the model should not be directly connected to the World, for instance, posting entries and comments on social media should be unavailable to it. Therefore, we can **include a link** in the response for quick **confirmation** of the generated content and its publication. This confirmation must be handled exclusively by code (not the model).
- **Self-referencing:** the system should have the ability to use its API, including affecting prompt elements or building dynamic tools in a "sandbox" environment (e.g., e2b). Self-referencing may also include issuing commands to itself, just like the user does.
- **Request for help:** some tools should allow for "suspending" execution until help is obtained from a human. This may occur due to lack of access to a service or exceeding the number of available attempts to perform a task.

In short: generative applications should adapt to the user and create value for them while minimizing the required engagement from their side.

## Summary

In this lesson, familiarize yourself primarily with the table structure from the events example. If you already have an idea for your agent (even a simple one), consider how you will organize the data it will work with. I'm not just referring to memory or information loaded from files, but especially message history, action history, and error handling mechanisms in a way that allows the agent to attempt to fix them.

We'll discuss the details in the final week of AI_devs 3.