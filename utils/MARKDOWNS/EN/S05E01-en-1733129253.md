![](https://cloud.overment.com/S05E01-1733049915.png)

References to AI agents have already appeared multiple times in previous lessons, but only now are we ready to fully address this topic.

Let's start by noting that the definition of an "AI agent" is quite fluid. In lesson S00E04 — Programming, I mentioned that we refer to systems capable of autonomously performing tasks. In the context of generative AI, we talk about an application where an LLM (Large Language Model) is connected with tools and long-term memory (referred to as Augmented Language Models, ALM).

Based on this definition, it can be assumed that practically all the examples we have discussed are a type of agent. This assumption is not incorrect, as we indeed talk about a "certain degree of agency." Thus, the term "agent" is not a noun but a verb describing the level of autonomy of the system. To be clear — this statement comes from Andrew Ng and is more of an opinion than a definition.

This lesson begins the final chapter of AI_dev’s, which aims to integrate everything we have learned about working with models and building tools. It is time to create a system capable of autonomous operation, requiring minimal human involvement.

## Main Assumptions of AI Agent

Unlike the tools we have built so far, an Agent can independently decide how to approach a task and **when to finish it**. This means that upon receiving a command, a loop is triggered, the exit from which is decided by the model (or a programmatically imposed limit).

In the `agent` example, there is logic containing such a loop. We see in it that the agent has its state, which is updated with each iteration. Meanwhile, the iteration begins with **planning the next move** and **activating a tool**. When the chosen tool becomes "final_answer," the loop is broken, and the agent sends a message to the user.

> Important: For the purposes of the `agent` example, the state is reset only after the Node.js server is restarted.

After sending a simple message "Hey, what's up?", the agent does not immediately respond but considers the appropriate tool selection. Since the question does not require additional actions, the **final_answer** tool is chosen. This decision breaks the loop (with the "break" command on line 52) and proceeds to generate a response.

In a situation where the user's query exceeds the agent's abilities, the "final_answer" action will also be selected, with the note that the command cannot be completed due to limited capabilities. It is worth bearing this in mind and designing the system so that it can handle scenarios beyond the scope for which it was created.

The `agent` example includes the option to use a second tool, allowing for searching selected web pages. This is the exact logic we discussed in the `websearch` example with slightly modified prompts. Below, we see that the request for the latest events from Anthropic and OpenAI was correctly executed and contains up-to-date information (as of the day of writing these words).

The aforementioned "state" is important as it allows for follow-up questioning. In the presented example, I asked for five entries from each source. The agent, already having the content of these pages loaded, used the available information instead of re-searching the network.

Ultimately, the `agent` example is a very simple implementation that presents only the most important mechanics. Specifically, these are:

- **Planning:** The agent aims to understand its current situation in the context of available knowledge and tools. Then it decides on the next step.
- **Memory:** The agent has memory that includes both exchanged messages and actions taken.
- **Tools:** The agent has tools at its disposal, which it can use when necessary.

The combination of "reasoning" and "action," which we refer to here, relates to the ReAct pattern involving **reasoning, observation, and action**. However, in our case, planning and observation occur as part of a single step.

This all leads us to the question: **How should an agent be built?**

There is no unequivocal answer here, but we have patterns and various work techniques available, which can be used in different configurations.

Ultimately, the primary goal will usually be for the agent to **break the task down into smaller actions, complete them, and combine the obtained result into a final response**.

## Agency Patterns

On the deeplearning.ai blog, there is a series of entries on agency patterns and their elements such as **Reflection, Tools, Planning,** and **Agent Teams**. If you have gone through previous editions of AI_devs, the first three of them are already familiar to you, but still, we'll now look at them from the agent's logic perspective. Now the significant difference is that they operate in a loop, which opens up space for autonomously solving complex problems. So let's take a closer look:

- **Reflection:** is the process of contemplating the current situation.
- **Tools:** involves deciding which tool to select and how to use the obtained result.
- **Planning:** involves setting a list of actions or deciding on the next step based on available information and the set goal.
- **Agent Team:** involves establishing cooperation between specialized agents who, communicating with each other, strive to achieve a common goal.

Before we move on, I will note that in this lesson we will focus on the most important topics of agent design. However, this topic is very extensive and is currently still in the exploration area. Practically every month, new publications appear, bringing something new or questioning existing work techniques. Some of them can be found in the Awesome LLM-Powered Agent repository.

## Reflection

Let's start with **reflection**, which for humans usually occurs in silence and is sometimes supported by tools (e.g., paper or a text editor). Such deliberation allows us to better understand a given topic and **increases the chance that we will make the right decision**.

Exactly the same goal is aimed for with reflection in AI agents. However, we know that language models in their current form have very limited "thinking" time when generating a single token. In other words, the model "thinks" **by generating consecutive tokens**. For precisely this reason, we applied `_thinking` properties when generating JSON objects or the `<final_answer>` tag when statements did not require a structured answer. However, reflection can take various forms and be much more elaborate than what we have gone through so far and **can even take the form of a separate agent**, an example of which is described in the publication "[Improving LLM Reasoning with Multi-Agent Tree-of-Thought Validator Agent]."

However, reflection is not solely aimed at generating additional tokens, but tokens that will **favor obtaining a correct answer**. We will want to reduce the risk of information appearing that will work against us.

By default, the "thinking" performed by the model is quite general, expansive, and contains much unnecessary content. The example below illustrates this well because the reflection is almost an exact repetition of the answer (i.e., the section separated by a new line), although the fragment shown here has been slightly trimmed.

> Fun Fact: Chain of Thought can also **negatively** impact the model's effectiveness for some tasks, as described in the publication "[Mind Your Step (by Step)]." It specifically talks about "zero-shot chain of thought," which is a situation where the model "thinks step-by-step" without more suggestions from us. It turns out that this approach does not work for some classifications, especially those that involve some deviations from the norm.

By comparison, below is a fragment of my discussion in Cursor about introducing corrections in the code. The model did not go directly to writing the code. First, it listed the elements it would work with, and then **pointed out key concept words**, such as Single Responsibility Principle or Dependency Injection, that might be important when performing this task.

In the `agent/WebService.ts` file, there is a `generateQueries` method whose prompt also contains a reflection element, but its content differs from the previous ones. It is concise, precise, and focused on facts available in the context.

The reason is the examples contained in the prompt, which the model imitates in its responses. Their content directly results from my experiences and from the goal embedded in the specific context of using the search engine.

Thus, when thinking about model "reflection," we must always be mindful of **supporting** the reasoning process by striving for content that **increases the chance of obtaining the expected result**. We will therefore want the model to **refer to keywords, concepts, or list the threads on which its attention should be focused**.

Only having influence over "thought" in the way visible in the last example can have a negative impact on model performance because it will not have the chance to explore paths we did not consider. It is therefore worth exercising caution.

A general rule that can guide the shaping of the reflection process is to consider **how we would approach solving a problem** and **how this translates into the model's capabilities**. In doing so, we notice which things need to be considered and what information the model needs.

## Tools

Having prepared tools with a common interface, which we discussed in S04E01-Interface, integrating them with the agent becomes relatively simple. For this reason, we have almost always ensured that the input data for each had the form of **a text query**, and the output data — a form of documents.

So far, we have had examples, such as `web` or `todo` where the LLM used tools. However, in both cases, the mechanics present were specialized in one area. But in the case of agents, this scope will be somewhat larger.

For a reminder, the `todo` example consisted of three actions — planning, execution, and response. Additionally, the planning stage generated not only the necessary steps but also the parameters needed to run each action.
This approach won't suffice when an agent is equipped with a greater number of tools, as the complexity of the prompt will not allow for effective answer generation.

However, in the `agent` example, this logic includes an additional step related to **preparing parameters** to launch a tool based on a user guide. This allows the **model's focus** to be on one task, which positively affects the precision of the statement.

Let's now look at what happens when the `agent` example is asked to find a film on "Game Theory" from Yale University:

- The agent considers that the lack of information about this film in its resources requires the use of the "web_search" tool, which it initiates with a simple query to a search engine.
- Within the "web_search" tool, a series of queries occur that seek to narrow the query to the indicated source and generate sub-queries if needed.
- Search results are converted into documents and added to the agent's short-term memory.

This all indicates that regardless of the number of tools, as long as their interface remains consistent, the agent's logic will be sufficient to effectively utilize each one of them.

Furthermore, in the `agent` example, unlike earlier ones, there is a possibility to **react to a potential problem**, and if the acquired information is insufficient, the agent might decide to continue searching or request human assistance. Thus, we have a degree of autonomy and flexibility allowing adaptation to the current problem.

Therefore, using tools by an agent involves:

- A planning stage where the model decides which tool to choose based on its name and description, as well as the information currently held.
- A stage of **describing** the parameters needed to run the tool based on instructions.
- A **tool launch** stage (which happens on the developer's side).

The most important things here are twofold: **precision of descriptions/instructions** and **properly provided context**. In other words, the model should leave as little room as possible for error. This is well visible in the prompt responsible for the planning stage, which contains context with available knowledge and a history of taken steps. Note:

- The "xml-like" syntax clearly separates context sections.
- Attributes provide context for each action. For example, the description for the websearch action looks like this — `This is a result of a web search for the query: "${searchResult.query}"`. It includes information about what query these entries are for.
- Values indicating **lack of results**, which may also suggest that the action failed, and the agent might inform us of this or decide on another attempt.

In short, in connecting the agent with tools, **precision** is key. Although it may seem obvious, the complexity of the dynamic context grows rapidly, making errors easy.

In our case, it's not needed now, but if the agent's skill list were very large, indexing them in the search engine could be considered. Then only a few entries would appear on the list of currently available tools, which would have a positive impact on precision. Moreover, examples of using these tools could appear in the prompt **dynamically**, further increasing effectiveness.

## Planning

In the `agent` example, the planning and reflection stages are combined. However, we will usually need to split this process into two separate activities. Then planning will focus solely on aligning **reflection/thoughts about the current situation**, **possessed knowledge**, and **available skills** to take the next plan.

In one of my projects, the planning stage (visible in the `next step` screenshot) is preceded by **three types of reflections**. The first is responsible for retrieving keywords for searches, the second indicates memory areas that need searching, and the third focuses on suggesting skills worth considering.

Generated results are passed to the "planning" stage as **suggestions** based on which the final decision on the next action is taken.

This clearly shows how individual components of the agent's logic can be divided into specialized steps that can be triggered only in selected situations.

Planning itself might also involve arranging a series of actions, not just choosing one of them. However, I don't use this approach because:

- **Variability:** Unforeseen situations may arise during actions that the agent should adapt to.
- **Focus:** At any moment, the agent considers only the next action, not the entire series.
- **History:** With each step, the agent already possesses information about the taken steps, so it can refer to them if needed.

The planning/decision-making stage is the most crucial of all actions taken by the agent. We must therefore ensure that the effectiveness of the prompt working here is as high as possible. We can do this in three ways:

- **Examples:** Few-Shot is the most effective way to enhance model performance and is the first thing we should consider when optimizing the prompt.
- **Verification:** An additional prompt focused solely on assessing the decision made is a demanding but very effective way to detect error.
- **Fine-tuning:** Once we have a functioning system and dataset, we can specialize the model in making the next step decisions.

And of course, we should always consider the best available model in this place.

## Memory Types

The agent's memory is typically divided into three categories:

- **Conversation history:** Covers both the current conversation and previous interactions.
- **Documents:** Includes both user-uploaded documents and tool results.
- **Long-term memory:** General knowledge about oneself, the interlocutor, and the surrounding world, typically includes information beyond the model's basic knowledge.

We're dealing with a database where entries are also added to search engine indexes. The module will usually serve as an additional tool from which the agent will pull like in the case of other skills, and its result will be a list of documents loaded to the context.

In terms of memory, it is also worth considering access through an API, allowing for access to individual entries, their update, or display in the user interface.

Memory is a source of knowledge, so we care not only about access to it but also about creating and updating it. This is a significant challenge because:

- We must ensure that new information is added in a way that allows for easy finding in the future.
- During updates, data might be lost or stratified, for example, by creating a new entry instead of updating an existing one.
- Searching existing memories is not always obvious, as the user's query may be insufficient to efficiently connect it with all relevant documents.

Therefore, when building memory, consider:

- **Structure:** Structuring memories based on predefined patterns allows narrowing searches later, as seen in lessons S03E02 — Semantic Search and S03E03 — Hybrid Search, facilitating reaching desired content. While it also serves as a limitation, agent memory typically focuses on specific areas, not general knowledge on any topic.
- **Context:** Memory can have one note serving as a reference point for the agent when using memory. For instance, the context might include a list of our projects, so if I mention Tech•sistence, the agent will immediately know to search for information in the "work" area. Such context thus forms the agent's knowledge **essence**.
- **Exploration:** Similar to the `websearch` example, searching memories requires transforming the original query, including generating additional queries, based on the mentioned context. Thus, a simple user message: "Hi, how are you?" might turn into a series of questions about the current environment, the user himself, or the agent's persona. Language models can generate such queries very effectively, thereby exploring held information.
- **Prompt Cache:** The latest possibilities of "caching" prompts, combined with large context windows of Gemini models, translate to entirely new possibilities of working with memory. In most cases, the agent will be able to load all available information into one prompt and query it multiple times. We must remember about the limited capacity to maintain the model's focus, which might cause this approach not to work in every case. However, this might be a problem soon resolved.
- **Agent:** Similar to the reasoning example, a specialized agent can be responsible for handling memory.

## Frameworks

Finally, observe the rise of frameworks on the market offering the opportunity to simply create agents, such as [CrewAI](https://www.crewai.com/), [LangGraph](https://langchain-ai.github.io/langgraph/), [Swarm](https://github.com/openai/swarm), [AutoGen](https://github.com/microsoft/autogen). At first glance, they seem to fully address many things we've learned in AI_devs 3 through a framework.

Below, we see a simple variant of an agent comparable to the `agent` example from the start of this lesson. Just 28 lines of code were sufficient to use OpenAI and FireCrawl to retrieve 3 entries from the Hacker News homepage.

Problems, however, begin when we must specify the task in more detail and tailor the agent’s behavior to our needs. The examples we've covered in previous lessons often show how important it is to care for details hidden behind the abstraction layer.

Perhaps an even greater problem with frameworks is their updates, which at the current pace of generative AI development often require changes in API. An example is the transition from "Function Call" to Tool Use, OpenAI's Real Time API appearance, or Anthropic's Computer Use. As a result, frameworks like LangChain have undergone several transformations, with many methods marked as `deprecated`, challenging even for applications built 6-12 months ago.

Ultimately, sooner or later, one of these frameworks will gain greater popularity and stabilize enough to be successfully used in production projects. Until then, it is much better to base projects on your solutions unless you're only interested in quickly building a prototype or PoC. In any case, it's worth observing the development of frameworks because they currently provide great sources of inspiration and can sometimes be useful in individual components of our agents.
## Summary

Looking at the concept of an agent, we are talking about logic enclosed in a series of loops, controlled by a language model, with additional constraints on the programming side.

Currently, due to the limitations of language models, we will likely focus on creating **specialized** agents, possessing a few to a dozen skills and relatively simple memory.

Nonetheless, the main elements of logic should be designed universally. This way, when we add a new tool to the agent, there won't be a need to introduce additional changes to use it.

It's also a good idea to build your own toolkit that we can connect to different agents. Some of them have already been built in previous AI_devs 3 lessons and serve as a good starting point. Ultimately, however, it's worth considering your own library.

And finally, I'll add that great creativity is indicated here. Although there are schemas, patterns, and best practices for building agents, it's worth knowing them and then creating your own.

If there's only one thing to take away from this lesson, it won't be building your own simple agent, like the example `agent`. Try to connect it with one or two tools and make it pass 10 of your tests correctly.

Good luck!