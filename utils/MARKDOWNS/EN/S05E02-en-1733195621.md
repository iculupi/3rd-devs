![...](https://cloud.overment.com/2024-12-02/s05e02-5ceadb39-0.png)

The ability to understand the current situation, identify a problem, and develop a plan to address it enables the undertaking of complex actions and the pursuit of a set goal, despite changing conditions. This is also a fundamental skill of an agent, on which everything else depends, and it's this skill that we will focus on in this lesson.

**Reflection** and **planning**, which we talked about in the previous lesson, are activities that form part of what we will be dealing with today. However, this time, we will look at the topic from a slightly broader perspective.

Both the general components and the details of implementation will differ depending on the system we are building. Therefore, to give context, let's outline the general shape of our agent.

Let's assume it will be an advanced assistant, similar to those we have built in previous editions of AI_devs (i.e., a personal assistant), but this one will have much more operational freedom. We have thus:

- **The ability to conduct conversation**, but this time including various "senses" (image/audio) as well as handling documents and files.
- **The skill of using tools**, but this time this ability is open-ended, and the agent can independently decide which tools to use. Among the tools will be: **memory**, **internet**, **file system**, **task list**, **Spotify**, and others, related to supporting us in daily work.
- **The possibility of remembering information**, but this time it will be a simple "memory" board.

These are, therefore, the main components of the agent, which we have already seen in the previous lesson. Now we can look at them through several scenarios:

- **Reminder:** The user asks for a reminder of previously saved information, e.g., a link to a video.
- **Action:** The user issues a command that requires knowledge about his preferences.
- **Assignment:** The user sends a file with a request to check its correctness based on internet data and then send the results by email.
- **Thought:** The agent sends a message to itself as a result of the task schedule, e.g., "I was supposed to remind the user about ...".

All these tasks, although they differ in the way they are executed, have common elements. Let's see how such an agent could look by focusing on one of the mentioned scenarios. 
## Overview of agent mechanics

In the `assistant` example, there is logic similar to what we discussed in lesson S05E01 — AI Agent, although this one is a bit more complex. For this reason, I have simplified most of the mechanics (tools, memory), allowing us to focus solely on the LLM-related part.

The main mechanics are as follows:

![](https://cloud.overment.com/2024-10-29/aidevs3_assistant_schema-0535f470-4.png)

It includes the following stages:

- **Initial reflection stage**/recognition, in which the agent seeks to orient itself in the current situation.
- **Planning stage**, where a **task list** necessary for execution is created before providing an answer to the user. This list can be updated.
- Still, at the planning stage, the decision on an **action** to be taken as part of the **current task** is determined.
- **Execution stage**, in which the data needed to launch the action or use the assigned tool is generated.
- The execution stage also includes the actual execution of the function and adding its result to the context.

Therefore, the differences compared to the `agent` example are as follows:

- The initial reflection stage allows for the initiation of the main mechanics, providing it with information not given by the user. Here prompts have the character of "inner speech," where the model is informed that the user does not see its statements.
- The planning stage involves dividing into **tasks** and **actions** within them. The difference is important because a task (e.g., reading and transforming a document) can fail during execution. Thanks to the `task > action` hierarchy, we can easily check which action failed and, if necessary, resume it without repeating all previous stages.
- The execution stage includes a separate step focusing **exclusively on generating the request object** needed to launch a tool/take an action. Here, the model's attention focuses solely on this one thing.

Such an agent configuration translates above all into greater flexibility and effectiveness due to higher quality context and the ability to change the plan during its execution.
## Example

The `assistant` example intends to demonstrate the main mechanics, and if we wanted to use it in conjunction with different tools, changes to the prompts would likely be necessary. However, the overall logic includes all the essential elements that will appear in most agents.

So run this example for the query I've set. Try to modify the values/information contained in the context and also the list of tools and check how the agent behaves. Pay attention to the changes needed and the fact that the agent may struggle to find itself in every situation.

The expected behavior is that when the user requests to turn on their favorite music, the agent must "remember" what music is involved and use this knowledge in further steps.

Below is a log of the successful completion of the task, done in four phases (including the initial one). The entire visible path was not coded by us but is the result of the language model's action combined with the logic written in the code.

![](https://cloud.overment.com/2024-10-30/aidevs3_example-357e853f-1.png)

## Persona

Starting from the first stage of "reflection", the topics of **environment**, **personality/general context**, and also asking a few questions to oneself appear in it.

This stage is important because even if the agent is connected to a database and can load information about us, upon starting the interaction, it does not have access to it and must "somehow" reach the knowledge required for further action.

After lessons on searching, we know that we can enrich the user's query through prompts in which the model asks itself questions to assist in memory exploration and loading the right memories.

For example, if I ask: **What's the weather like now?**

The agent can ask an auxiliary question: **Where is the user currently located?**

In this way, the agent reaches the necessary information and can use it in conjunction with the tools/skills it possesses.

However, asking such questions will not always be so obvious and can lead to generating queries that do not make sense or, worse, lead to data irrelevant to the given context.

Example: **Explain to me how streaming works?**

"Streaming" is an issue that has many meanings, and the way of asking questions on this topic will differ depending on the context and will be different for a programmer than for an internet creator. And since the agent initially does not have it, the risk of generating incorrect queries is high.

For this reason, it is worth creating a so-called "general context" or the mentioned "persona" of the agent, in which the most important information about it is stored, which does not change too often over time. The same can apply to knowledge about the user or information from the environment. Initially, one might think that the entire general context should be available, but the content of a lengthy note can unnecessarily distract the model. So it's worth doing something about it.

In the `assistant` example, starting the interaction includes a stage named `thinking`, whose goal is:

- Compressing information regarding the environment and knowledge about oneself and the user. In other words, the agent takes from the "general context" only information it deems helpful. Thanks to this, we do not need to load all of them in further steps.
- Asking themselves a few initial questions about **memory areas** that may be beneficial to scan and **indicating potential tools/skills**

![](https://cloud.overment.com/2024-10-29/aidevs_context-5718c022-5.png)

In other words — **the agent tries to orient itself in a given situation** and make some initial assumptions. However, these assumptions are not final and will only serve as a guide in further steps.

This approach also refers to the "thinking time" for the model, about which we have talked many times. Below we see an example for the request to "turn on favorite music". We see that the agent noticed the music currently playing and even assumed that it could possibly be one of my favorites. Additionally, it loaded general information from the general context/personality about "loving classical rock".

![](https://cloud.overment.com/2024-10-29/aidevs3_thoughts-3607f1f5-7.png)

We thus have preliminary clues allowing us to continue the response, but at the same time, we do not have all the necessary details required to perform the action and provide the answer.
## Initial recognition

Apart from listing essential segments about the environment and general context, the agent also asks itself a series of questions regarding **tools** and **memory**. However, these queries are not carried out **simultaneously** with the previous ones because their results are utilized for better formulating questions.

This is an important issue because we always need to consider **what information the model requires at this stage** at the given moment.

Below we can see that queries from the "Memory" and "Tools" categories are no longer general but concern (in this case) classical rock. Again, this allows for better memory search.

![](https://cloud.overment.com/2024-10-29/aidevs3_selfqueries-68f98888-9.png)

The aim of the above queries is to **direct the model's attention** toward a specific direction and generate quality content, which will be added as context to further queries.

Also looking at it from a broader perspective, we take steps that utilize the capabilities of language models to enrich the original user query with as many details as possible.

The most significant challenge at this stage is ensuring that the model **does not engage in user response**, but focuses solely on generating "thoughts." Pressure to achieve such behavior is placed directly in the system prompt.

![](https://cloud.overment.com/2024-10-29/aidevs3_internal_dialogue-501789e1-9.png)

## Current state of the agent

In the `assistant` example, the state of the agent is also more extensive than in the example from the previous lesson. It includes:

- Settings: number of available steps and current task and action.
- General context/environmental data that can be loaded from the user profile.
- List of skills/tools along with an instruction manual.
- List of "thoughts", which we just discussed.
- List of documents loaded and memories invoked.
- List of messages from the current conversation.

![](https://cloud.overment.com/2024-10-29/aidevs3_state-a54dc561-9.png)

The state structure will differ depending on the agent, but its main premise is to store all information related to its operation, in the context of the current conversation. Data stored in the state will be available practically at every stage of the agent's logic, and individual values will be added as context to the prompts.
For example, at the task-setting stage, we only see the name and description of each one, without details about the actions taken (although adding a list of them with the status of completion is advisable).

![](https://cloud.overment.com/2024-10-29/aidevs3_tasks-8c87bbf5-2.png)

In contrast, in the prompt responsible for deciding the next action, the provided context also includes **the results of previous steps**.

![](https://cloud.overment.com/2024-10-29/aidevs3_actions-12494a00-2.png)

In addition to accessing information, it's also crucial to ensure the logic responsible for **updating the status**. Here, as we learned from previous AI_devs lessons, whenever possible, we will support the model with logic encoded in the program.

Thus, for the prompt where agent tasks are created/modified, I programmatically block the ability to edit those whose status has been set to "completed".

![](https://cloud.overment.com/2024-10-29/aidevs3_state_management-f6b54ddd-d.png)

The agent's state serves another important role, concerning the maintenance of "awareness" of what has occurred between previous messages. Moreover, the decision of which of this information to load can also be made at the "initial assessment" stage, which is conducted before the loop begins.
## Memory

The agent will typically have **long-term memory**, which it can recall at any time, and **short-term memory**, which will be accessed only within the given thread. We're talking about both **discussed documents** or **"memories"**, and sometimes even the message history. However, in this last case, conversation content or even single commands aren't valuable for future interactions. Therefore, it's worth establishing clear boundaries between the content to be permanently stored in memory and that which is needed only at the moment.

In the `assistant` example, there's a section where the agent retrieves memories about the "user's favorite music". In this case, adding its content to the system prompt is not an issue, and is indeed necessary for further operation.

![](https://cloud.overment.com/2024-10-30/aidevs3_memories-1bc9b62d-d.png)

However, it would be enough for the task to involve loading web page content, and it might quickly turn out that the prompt context is filled with information that largely distracts the model's attention. In such a situation, it may be worthwhile to consider using "information retrieval" prompts like those operating at the "reflection" stage.

Alternatively, we can compress the document to a few descriptive words. In other words, we cannot forget that we have a language model at our disposal, which can transform content in various ways and adapt it depending on the context. Thus, even if the list of memories is extensive, we only load those fragments that are needed at a particular moment.

Also, by combining the knowledge from lessons S01E04 — Optimization Techniques and S03E03 — Hybrid Search, we can build a long-term memory system for the agent. However, besides effective searching, we must also ensure the loaded context is limited to the essential minimum.

## Development Plan

The mechanics of the `assistant` example are very similar to those used in one of my projects. In that case, the agent has several tools, within which there are about 70 different actions. It's quite flexible but should primarily be treated as a reference point.

However, this doesn't mean that every agent must have such a range of skills, and its logic can be much simpler. On the other hand, an agent may have its interface or not interact with the user at all, except for the need to verify the results of its work.

An agent can also take the form of **a single file**, and its "specialization" can be broad enough to allow it to "build itself". I'm enclosing these expressions in quotes for a reason, as I mean a small, hobbyist project that nevertheless presents an important idea. This refers to the [Ditto project](https://github.com/yoheinakajima/ditto).

![](https://cloud.overment.com/2024-10-30/aidevs_ditto-cf7aa9fb-0.png)

Ditto is also not an agent completely detached from reality, as the project [aider.chat](https://aider.chat/) demonstrates the model's high effectiveness in working with file systems and implementing application functionalities.

In building agents, often the primary problem is the idea itself. Hence, it's worth observing the market for projects, particularly Open Source ones, that can inspire us to create our agents. On GitHub, you can find repositories containing extensive lists of such projects. An example is [Awesome AI Agents](https://github.com/e2b-dev/awesome-ai-agents?tab=readme-ov-file#open-source-projects) developed by e2b, but there are many such resources.

![](https://cloud.overment.com/2024-10-30/aidevs3_inspiration-55c5280a-d.png)

A very likely scenario is that creating a given agent will not be feasible or its effectiveness will be too low to be meaningfully applied. However, we must remember that the vast majority of examples we've gone through until now were impossible to execute a year ago.
## Summary

The logic of the agent we saw today can be combined with practically **any tool we've discussed so far in AI_devs 3**. Furthermore, they can be used in virtually any order, opening up quite exciting possibilities.

Below is a slide from one of my workshops that can be compared with what we've learned so far in AI_devs. Specifically, with **programming tools** that the model can use, a relatively simple application will be able to address different processes. Specifically, an agent equipped with Internet access, the ability to read and create files, and send emails will handle almost completely different tasks.

![](https://cloud.overment.com/2024-10-30/aidevs3_options-a1ad3007-6.png)

All this should draw our attention to somewhat different principles of application design. Until now, most of us have been developing software with very narrow specialization. Here, we move to a higher level of abstraction, creating solutions that don't precisely describe the logic of specific tasks.

We should take only one thing from this lesson, and it's the experience of how `assistant` can both **adapt to various situations and fail to do so** independently. This awareness helps temper the enthusiasm for fully autonomous agents (at least for now) in favor of specialized ones that require human verification.