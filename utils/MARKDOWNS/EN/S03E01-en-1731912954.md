![cover](https://cloud.overment.com/S03E01-1731754226.png)

At least several times, we have encountered issues with input/output token limits. Although current SOTA models are capable of processing quite substantial amounts of content, it still doesn't allow for free work with extensive data sources.

It is also worth knowing that in the publication [Leave no Context Behind], possible infinite context windows are discussed. According to the authors (Google DeepMind), their approach is effective in practice on a scale of 1-2 million tokens, which we can experience ourselves today by working with Gemini models. Meanwhile, competitors' models maintain their limits at the level of 100-200 thousand tokens.

Just a few months ago, one could say that even an infinite context window wouldn't be useful due to high costs. Today, practically everyone offers prompt caching, which reduces not only costs but also the time required to process a query.

However, the missing pieces of the puzzle are the output token limits and the ability to process other content formats. On the [Firebase](https://firebase.google.com/docs/vertex-ai/gemini-models) page, we find a comparison of all figures for Gemini models, which can be considered leaders in this regard.

![Image](https://cloud.overment.com/2024-10-05/aidevs3_tokens-958b3bb5-b.png)

The limit of output tokens is particularly troublesome in the case of **document transformations** such as proofreading, translation, or detailed review. For such tasks, it is necessary to divide everything into smaller fragments (known as Chunking) and then process them individually.

The technical capability to process extensive content is not the only element to consider here. Equally important is the ability to maintain attention, illustrated by "Needle in the Haystack" tests (more about this in [Lost in the Middle](https://arxiv.org/pdf/2307.03172)). It is about the ability to retrieve specific information from the provided context. According to various sources ([1](https://medium.com/@lars.chr.wiik/openais-gpt-4o-vs-gemini-1-5-context-memory-evaluation-1f2da3e15526), [2](https://nian.llmonpy.ai/intro)), the current best results are achieved by OpenAI with the GPT-4o model. However, my experiences rather suggest that even in his case, we are talking about precision at the level of 85-95%.

![Image](https://cloud.overment.com/2024-10-05/aidevs3_haystack-ed225df5-1.png)

The problem is that the test of finding individual pieces of information in text is extremely limited and does not test skills related to **following instructions** or **"understanding"** content. These areas also remain challenging for current models.

Finally, providing content to the model is an activity that is not limited solely to adding more files or web page content. Often, we are talking about dynamic data and the need to provide context that allows the model to better handle new information in practice. For example, if I say, "I spent a lot of time today at Tech•sistence," from the model's perspective, it's unclear whether I'm talking about a project, a new game, or a book because this information is not found in its base knowledge.

## Concept of Documents

The topic of preparing content for the model has already been discussed in lesson S01E02 — Context. Now, however, we will address it again, not only in theory but also in practice.

A document is an object consisting of the main content and metadata that describe its features and provide context. In the context of LLM, it is usually a small fragment of a longer piece of content from an external source, which we add to search engine indexes for Retrieval-Augmented Generation or other scenarios requiring the retrieval of specific information sets.

An example of a document generated based on one of my entries is shown below (the `text` property has been truncated by me and is typically somewhat longer). The metadata includes **source information**, **data that allows locating the document's original source**, its unique identifier, and links that in the content have been replaced with placeholders.

![Image](https://cloud.overment.com/2024-10-05/aidevs3_document-efc54143-8.png)

The structure of metadata will depend on the specific case, but the general assumption is to include information that:

- allows indicating the source and neighboring documents
- allows outlining the context (e.g., which file the document originates from)
- allows effective filtering of documents (e.g., based on date, permission roles, or category) — these metadata are not included in the above example
- allows compressing the content of the document (an example is links, which I replaced with placeholders)

Therefore, it is worth thinking about documents in such a way that when we build them based on a file or database, we must generate them so that the model has sufficient context about them and so that the search engine can effectively reach them — both of these things will become fully comprehensible in further lessons on search.

The content of documents is usually generated through Text Splitting or is generated by the model or directly by a human. Although frameworks such as LangChain seem to promote dividing content based on length or possibly subsequent special characters (known as recursive text splitting), these techniques generally work in practice, although according to some, additionally applying "overlap" (overlapping documents) can be effective, but it requires overlapping a large part of the documents (e.g., 50%).

However, let's see this in practice using an example quote from The Lord of the Rings.

In the first example, we divide the text based on the number of characters, specifically 250. We get seemingly well-looking documents, but when looking at each of them individually, even a human would sometimes find it challenging to use them in practice.

![Image](https://cloud.overment.com/2024-10-05/aidevs3_character-2b36617d-b.png)

We see that in the example sentence "Three were given to the Elves immortal, wisest" ... "and fairest of all beings," applying the mentioned "overlap" would help maintain context. However, the chance that this will happen for all possible documents is low.

The situation significantly improves when the text is not divided based on length but rather on paragraphs or possibly sentences. This is known as recursive text splitting, in which we use a set of special characters (e.g., double newline, newline, period/question mark/exclamation point, or space). In this case, the loss of context for a specific fragment of text is quite low, although still possible.

![Image](https://cloud.overment.com/2024-10-05/aidevs3_split-63646f6b-f.png)

I mentioned that we will deal with document retrieval in further lessons, but we must pause on it now. The reason is an article from the Anthropic site discussing so-called [Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval).

To understand it, just take a look at the two diagrams below. The first represents a typical RAG system, in which we create documents (chunks) for a dataset (corpus) and index them in a classical search engine and vector database. Later, based on a user's query, we refer to the indexes and retrieve relevant documents for LLM context.

![Image](https://cloud.overment.com/2024-10-05/aidevs3_standard_rag-fb37a776-f.png)

The problem is that the documents in this case can still lose their context, and worse, they might not even be found, even despite hybrid searching.

Anthropic suggests an additional step at the document creation stage. It involves processing each of them through an LLM and a prompt that gives it context based on the entire document. This seems to be a costly operation, but thanks to prompt caching, the situation changes completely. Moreover, combining this with models like Google DeepMind's Gemini allows processing quite large datasets.

![Image](https://cloud.overment.com/2024-10-05/aidevs3_contextual-3ebe6fbe-8.png)

The prompt mentioned is shown below. As you can see, it is not particularly complex, but its structure is not accidental. The fact that the entire document is at the beginning of the prompt allows utilizing the cache memory mechanism.

![Image](https://cloud.overment.com/2024-10-05/aidevs3_contextual_prompt-d8e4eb00-8.png)

For our example quote, the first document is described in the way shown below. Such content combined with its content can be added to search indexes.

![Image](https://cloud.overment.com/2024-10-05/aidevs3_document_context-886ea91c-1.png)

## Advanced Text Splitter

In the `text-splitter` example, there is a class containing the logic that I mainly use in conjunction with content saved in markdown format (including pages) and YouTube video transcripts. The code therein is quite complex as its task is not only to split the content but also to perform this task with the smallest possible number of token recalculations to increase efficiency. This logic is probably not error-free, but it is the best I have managed to develop. So if you find any errors there, feel free to share your observations in the comments.

The example simply loads the content of three different documents: unstructured text, a YouTube transcript, and an article in markdown format. The splitter's task is to go through the content and generate a list of documents whose length will not exceed the allowable limit but will not be too short either. Only the documents at the very end are exceptions, which inevitably may differ significantly from the rest.

![Image](https://cloud.overment.com/2024-10-05/aidevs3_splitting_test-965789f4-c.png)

You can test this splitter's operation on any article or text, preferably in markdown format. We will use it extensively in the course's later parts, so it is worth familiarizing yourself with it and the generated results.

## Working with Unstructured Data

The work techniques we learned during lessons such as S02E04 — Connecting Multiple Formats or S02E05 — Multimodality in Practice, which allow transforming content, are also useful when working on documents created from unstructured data. I am referring to both individual documents whose content must be processed and external information sources from which we need to extract only the data that actually interests us.

In the `unstructured` example, the model's task is to prepare a list of documents containing a list of tools appearing in my Tech•sistence entries. These are usually quite extensive articles, so generating documents based on the original content would include a lot of content that, in this case, is not needed. Therefore, using prompt caching, I first compress the article to focus only on the necessary data.

![Image](https://cloud.overment.com/2024-10-06/aidevs3_extract-ce1cab77-a.png)

I turn to the splitter only secondarily, receiving documents focused solely on the relevant content for us.

![Image](https://cloud.overment.com/2024-10-06/aidevs3_narrowed_context-8fe06240-3.png)

Alternatively, we might want to generate documents for each tool individually. As shown below, this is also possible and, moreover, allows us to further expand their content.

![Image](https://cloud.overment.com/2024-10-06/aidevs3_tools-80cb7caa-0.png)

Thus, we see that content processing can rely on advanced transformations that we have learned so far. Now, using the possibilities of prompt caching, we can do this much faster and cheaper.
## Categorization and Filtering

Our initial encounters with Retrieval-Augmented Generation, embedding, and vector databases suggest that the documents generated above can be effectively searched based only on a similar description of tools. Just a reminder that in the search process, we usually limit the number of returned results. This means that if I inquire about a "writing tool," the content of that query will be associated with descriptions of tools. The search engine will return, for instance, the 10 closest matches, from which the model will choose 4 — Notion, iA Writer, Paper, and Obsidian.

In other words, for precise queries targeting specific documents, the effectiveness we achieve will be high as long as the tool descriptions are sufficiently good.

![](https://cloud.overment.com/2024-10-06/aidevs3_simple_query-bb8069c4-5.png)

But if our query is very general, e.g., "list all applications from the database," the correlation between the query and the descriptions either doesn't exist or is significantly lower. Worse yet, limiting the number of returned results will mean we receive a maximum of 10 records.

For this reason, it's necessary to consider additional categories and tags that will aid in reaching documents even when semantic searching proves insufficient.

Thus, in the above-discussed example, we could add to each document a `type` set to `tool` and `tags` linking, for example, note-taking or task management applications. Both pieces of information will allow for expanding the process of accessing data and filtering results.

In practice, we will still engage with this, but I mention it now because it is an extremely important element in planning the structure of documents and strategies for storing and accessing information.

## Summary

The topic of document creation forms the foundation upon which we will build practically all other tools. As demonstrated even by the examples we went through, it is crucial to plan their structure with particular attention to the process of searching and delivering to context.

So if you are to take away only one thing from today's lesson, it won't be the example `text-splitter` and its logic of dividing content into smaller fragments. With the help of LLM, you can easily translate it into your programming language or even develop it with new possibilities.

As an additional thread, I draw attention to the mentioned Anthropic article on [Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval), which is likely to have a huge impact on the quality of AI agents you develop.