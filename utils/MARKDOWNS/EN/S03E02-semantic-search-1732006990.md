![](https://cloud.overment.com/S03E02-1728214829.png)

The topic of vector databases and semantic search has been mentioned several times, but only now will we take a closer look at it. I'll just add that introductory elements about embedding or configuring qdrant appeared in lessons S01E05 — Production and S01E04 — Optimization Techniques.

Searching is a key element of the process of linking LLM with external data, and it is from this process that the quality of the generated response depends. However, it turns out that achieving 100% accuracy has not yet been accomplished by anyone, and we can expect errors at a level of 3-6% and that's only for content retrieval alone, not counting errors of the model itself.

The actual effectiveness of the system will also depend on the model itself, the dataset, and the processed queries. The following graphic from [Anthropic's article](https://www.anthropic.com/news/contextual-retrieval) additionally shows that we must also look towards combining different search strategies.

## Semantic Search

We already know that embedding is a method of representing data through numbers that usually aim to describe their meaning for analysis, as well as searching. We usually store them in the form of objects, combining the embedding itself and metadata. These objects are called points or documents and are stored in a vector database, which is essentially a search engine.

To understand the principles of semantic search, let's look at the `embedding` example where test data in the form of names of several popular companies and my newsletter are found, as well as a few simple queries. At first glance, it is clear that matching these words based on their spelling is not possible, and traditional searching will not help us here.

After running the example, it turns out that Qdrant correctly matched Tesla with a car manufacturing company and associated Apple with MacBook. This was possible because the model used for embedding effectively captured the meaning of this information, which allowed for successful semantic search.

However, in the two remaining cases — associating "Meta" (the new name of the company) with Facebook and Tech•sistence with the Newsletter failed because `text-embedding-3-large` lacks information to describe the meaning of these terms.

Interestingly, exactly the same logic implemented with the [jina-embeddings-v3](https://jina.ai/embeddings/) model allowed for the correct association of the third query as well, but made a mistake with Tesla.

The situation changes dramatically when we enrich our data with category descriptions. The model no longer needs to rely solely on "understanding the company name," allowing for the correct association of all queries with the appropriate data.

It's also worth understanding that depending on the model we use to generate the embedding, we will be operating with a different number of dimensions, and this number will remain constant for our entire collection. Furthermore, the embedding must also be generated with the aid of the same model, so we must choose wisely.

In the `/embedding/points.json` file are also stored the pieces of information documented in Qdrant. The `vector` property includes a list of 1024 numbers generated by the `jina-embeddings-v3` model, which describe the content's meaning. This means that **whether the document is a single word or consists of several sentences, the model has the same amount of space to describe them.** It's worth keeping this in mind and keeping each document as short and focused on individual information as possible. Additionally, we must also consider the contextual limits for both the model generating the embedding and the model that will be processing it.

At this stage, the following illustration, originating from one of the earlier lessons, should be perfectly clear.

So, every query to the vector database must be converted to embedding, as only then can we compare it with the data stored in the collection. **However, this is not everything we need to know about semantic search.**

## Vector Database

In the context of a vector database, I almost always refer to Qdrant. This does not mean it is the only option, nor that the others are not worth attention.

Just like with any other application component, the decision to choose a vector database will depend on the specific project. Sometimes it will be more justified to use a free solution, and at other times a commercial one offering high availability.

An interesting comparison can be found on the [Picking a vector database](https://benchmark.vectorview.ai/vectordbs.html) page, which includes popular options.

Qdrant appeals to me due to its very friendly API and sensible free plan, which does not require any additional configuration. I simply have one cluster at my disposal, sufficient for development and small projects.

Within the cluster, I have defined a single collection where I store **all information** for a given application. However, individual documents/points have **metadata** based on which **I filter them at the search stage**.

Having one collection is usually recommended, as is having one database for one application. Only in specific cases, such as client requirements or application logic, does the need arise to create new collections or completely new instances.

It may also be quite reasonable to consider the choice between `pgvector` and `sqlite-vec`, which enable storing embeddings and searching directly within PostgreSQL or SQLite databases, versus a vector database like Qdrant. Here, the decision may hinge on search tests, application scale, and the required performance.

Summarizing it all, we are interested in:

- Choosing a vector database or an extension for the database (Qdrant)
- Choosing a model for embedding ('text-embedding-3-large')
- Defining the structure of the collection and the documents contained within

For ease, I have noted options in parentheses that are adequate for a start and with which we may stick longer.

## Designing Collections and Documents

We discussed document preparation in lesson S03E01 — Documents. Now let's look at it from the perspective of the vector database collection, specifically **filtering and loading content**.

The `embedding` example discussed earlier demonstrated that semantic search is not always precise. This problem grows significantly when more data appears in the collection because, depending on the context, we may be dealing with noise obstructing access to desired content.

I have already mentioned that documents/points added to the collection must have metadata used for **filtering** already at the search stage, not after. This is also one of the things to check when choosing a vector database.

We must also note that embedding is irreversible. Although the publication "[Information Leakage in Embedding Models](https://arxiv.org/pdf/2004.00053)" describes a method for recovering individual pieces of information from them, it does not have practical application from our perspective.

Therefore, **we must always store original information** and should **not** rely solely on the vector database. The reason is the difficulty of accessing information, for example, for listing all records stored in the collection.

Thus, except for situations where we are sure that indexed data will be needed **only** for search purposes, at least partial **synchronization** of data between a classical database and a vector database will be required. Records must therefore have a common identifier, and the application must account for logic ensuring data consistency.

In the vector database, we can store embeddings describing text meaning, but also those that describe images. The basis of the search process will remain the same, but we must remember to avoid searching for two types of content. The problem associated with this is well illustrated by a post from Jina AI.

Summarizing this part:

- Each document must have its **unique identifier** (UUID)
- Documents **should** be synchronized with a classical database
- Metadata **must** have properties allowing for **search narrowing**
- Metadata **may** have properties specifying their availability (e.g., user role or subscription plan)
- The document content should be as monothematic as possible so that its meaning description permits future query matching
- The length of the document should not exceed the context limit of the embedding model, and the model processing its content (I mean both input and output context limit)

## Search Strategies

If you already have any experience working with vector databases, the following scheme (source: https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6) should be familiar. This is the so-called "naive RAG," which involves dividing large content into fragments, indexing them, and later searching based on the user's original query.

Such an approach quickly reveals its drawbacks because the original user query won't always suffice to correctly find the data.

In the `naive_rag` example, we have the same logic as in the `embedding` example, but I changed the data to quotes from Jim Collins' and Simon Sinek's books. **NOTE: Before running this example, delete the "aidevs" collection from Qdrant**.

Despite having only a dozen records there, our RAG could not indicate information allowing answering the question: "What did Sinek say about working with people?" even though the query is quite precise.

And this is where advanced techniques related to both data indexing, filtering, classification, and query augmentation come into play. They can be practically tested in a simple version in the `better_rag` example.

Firstly, I ensured that the dataset contains information about the book's author. The author was also added as a property in the "metadata" object of the documents added to the vector database.

Next, the logic now includes an additional step responsible for classifying the query based on the author or authors. This allows for creating a **filter** for the searched documents.
![Image](https://cloud.overment.com/2024-10-07/aidevs3_better_rag_logic-77a50b08-4.png)

And it translates to a result in which even if we ask about both authors, we receive a correct answer.

![Image](https://cloud.overment.com/2024-10-07/aidevs3_better_rag_search-2c61aa64-d.png)

However, the problem arises with questions that require answers based on a larger number of documents. Limiting results to only three will prevent us from accessing all the relevant ones.

The first solution that comes to mind is increasing the limit while applying filtering based on the `Score` parameter. Although this may address part of the problem, it is certainly not a sufficient solution because a document may not be close in meaning to the query yet still contain valuable content.

Therefore, we will involve a model in the assessment of whether a given document is relevant or not. This is the so-called re-rank process.

In our case, it involves processing each returned document (I increased the limit to 15) through a prompt specifying the document's usefulness. I added the following logic to the `rerank` example.

![Image](https://cloud.overment.com/2024-10-07/aidevs3_rerank-f259de89-f.png)

The effect is that despite increasing the limit of documents returned by Qdrant, I achieved the best result so far with only **two documents!** This means if their content reaches the model's context, we won't distract its attention with unnecessary information.

![Image](https://cloud.overment.com/2024-10-07/aidevs3_rerank_result-92b0c381-2.png)

However, this is not the end of optimization, as we have not yet dealt with **enriching/transforming** the query itself. Because if a user asks a question in a way that makes it difficult to immediately infer the meaning, we could engage a model to try and fix it. This, however, will be addressed later in the course, but you can consider this possibility now.

Meanwhile, summarizing this part:

- Data that enters the database index must include metadata allowing for filtering
- Filters can be set programmatically (e.g., access rights based on a subscription plan) or by the model (e.g., determining the category about which the user asks a question)
- The user query can be enriched, expanded, or refined to increase the likelihood of accessing relevant documents
- Documents returned by the search engine can be evaluated by the model for their usefulness for a given query

It is easy to guess that the above concepts don't exhaust the topic of search strategies, but they provide a good foundation.

Many examples and publications about different approaches related to both data storage and the process of searching and delivering to the model can be found online under "Advanced RAG patterns." Examples include a [Pinecone blog post](https://www.pinecone.io/learn/advanced-rag-techniques/), a [LlamaIndex blog post](https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b), or a [post shared by Amazon](https://aws.amazon.com/blogs/machine-learning/advanced-rag-patterns-on-amazon-sagemaker/).

The valuable graphic below comes from the mentioned LLamaIndex post and presents a variety of concepts that outline the overall perspective of possibilities at our disposal. We will still use some of them in practice.

![Image](https://cloud.overment.com/2024-10-07/6cfb645c4e5dfea8f3e5a590c3d2bc1cbfcfead3-5818x7805-4955e848-f.jpeg)

## Limiting LLM Engagement

Despite the increase in inference speed, involving the model in application logic can significantly slow it down. We often don't need the reasoning capabilities the model offers, yet we still deal with natural language, which prevents 'encoding' logic.

In the `semantic` example, I added a dataset in the form of tools that our agent could potentially use. Below we see that as a result of its action, queries like 'play music' were correctly associated with corresponding tools.

![Image](https://cloud.overment.com/2024-10-07/aidevs3_semantic-343665ef-c.png)

It turns out that such matching is handled by the vector database, and the schema itself is termed a "Semantic Router," which is also presented [in the repository of the same name](https://github.com/aurelio-labs/semantic-router).

I always chose LLM-based logic, but I mention this possibility because in some cases, it may be sufficient. Of course, I again point out the problem with matching keywords or terms that are not known to the embedding model.

## Summary

After this lesson, it should be clear to you that a vector database is a crucial element of an application where natural language processing is needed. At the same time, based on the limitations I have shown, we also know that it is not the **only element**, and we must support it either through LLM, classic databases, classic search engines, or programming logic.

If you remember only one thing from this lesson, focus on the `rerank` example and understand its components, as each of them will appear in subsequent lessons. You can also consider scenarios where the model may not be able to find relevant documents and how you might change that.

Good luck!