![](https://cloud.overment.com/S02E03-1731372201.png)

We had the opportunity to witness the capabilities of generative AI in text and audio processing, as well as image interpretation. Now we'll see what options we have in terms of manipulating and generating new graphics and photos.

Graphic design and working with photos haven't been directly associated with programming until now, except for developing tools for this industry or building advanced interactions with HTML Canvas and WebGL. It seems, therefore, that in the era of generative AI, designers or illustrators should be interested in tools like Midjourney or Stable Diffusion. However, it turns out that's not entirely the case.

An example is the [picthing](https://pic.ping.gg/) tool, which allows for effective background removal from photos, created by the `Theo — t3.gg` channel's creator. While background removal itself isn't new, we're talking about a significant quality improvement here. Above all, this project is an example of how generative AI can be used to build useful products.

Another example is [Pieter Levels](https://x.com/levelsio) with his projects [PhotoAI](https://photoai.com/) and [InteriorAI](https://interiorai.com). They address specific problems and confirm their usefulness through statistics published by Pieter.

I mention the above projects to outline potential areas we can focus on and that may encourage us to take an interest in generative graphics. Especially since we don't necessarily have to create new products right away, but can even use selected models for functionalities of applications we are already developing.
## Current Image Generation Capabilities

The development of generative graphics brilliantly illustrates the progress made over the last 2 years. The graphic below, from the entry [Comparing AI-generated images two years apart — 2022 vs. 2024](https://medium.com/@junehao/comparing-ai-generated-images-two-years-apart-2022-vs-2024-6c3c4670b905), clearly shows that in 2022, it was difficult to discuss serious applications of these models. Today, it looks completely different, although there are still many limitations.

One of these limitations is the model's ability to generate image elements such as text, hands, teeth, or reflections. And while currently available models like Flux are getting better at these tasks, it's still difficult to achieve consistent results.

Following instructions generally yields average results, as I wrote in the lesson S01E03 — Limits. Therefore, the more complex the prompt describing the graphic, the lesser the likelihood of obtaining a result that meets expectations.

Nevertheless, the quality of generated images is already very high. Although achieving the desired effect requires numerous attempts, it is certainly possible. Moreover, we might not always be interested in creating a comprehensive image, but rather single elements (e.g., textures or backgrounds) or editing an existing graphic or photo. Below is even a simple example of ComfyUI, which replaces my face in a photo generated in Midjourney.

From a programming perspective, we will be interested in the availability of models via API or their standalone hosting. In the former case, our attention should be drawn to services such as the aforementioned Replicate platform, as well as [Leonardo.ai](https://leonardo.ai/). In the latter case, [RunPod](https://blog.runpod.io/how-to-get-stable-diffusion-set-up-with-comfyui-on-runpod/) or similar platforms offering GPU access may interest us.

Apart from the image manipulation itself, it is also useful to use templates based on which we will generate graphics. This is usually necessary for marketing purposes, e.g., generating advertising creations, blog article covers, or newsletters. We usually need the same creation in different formats, the manual development of which is very time-consuming. Below is an example of templates for event and course covers published on eduweb.pl. To generate a new set, it is enough to replace the photo and text in the main component, and the changes will be reflected in all other instances.

Current controllability of graphics-generating models is quite limited, but prompts consisting of both text instructions and reference graphics can be developed. This allows for maintaining style consistency, which is often required in the context of brand tone or project requirements. Below is an example of covers I used in one of my projects.

Significantly greater controllability can also be achieved in ComfyUI, but the actual impact on the generated graphic depends on the workflow configuration and our needs.

In summary, models from the generative graphics area significantly enrich manipulation and image creation capabilities, which until recently were impossible or very limited. This allows us to:

- Scale images while maintaining (limited) details
- Scale images by enlarging the frame (an example is Generative Expand available in Photoshop)
- Automatically remove backgrounds, even in challenging cases (e.g., hair, shadows, droplets)
- Remove selected elements from a photo (e.g., the background), replace them with others, and combine multiple photos
- Generate consistent images in line with brand requirements outlined in the brand book
- Use existing graphics for automatic generation into many formats
- Create approximate visualizations based on descriptions and reference graphics
- Animate graphics into video form (e.g., thanks to [Runway](https://runwayml.com/), [Heygen](https://www.heygen.com/), or [Kling AI](https://klingai.com/))
- ...and much more.

## Techniques for Designing Prompts for Models

Generating graphics, like in the case of LLM, requires writing a prompt, although their content differs significantly, as we need keywords and flags controlling settings instead of full instructions. This is visible in the prompt example from Midjourney in the public gallery of user `kvovoorde`. It shows a detailed scene description with keywords such as `shiny`, `dark blue`, `warm weather`, `natural lighting`.

Another example of equally great graphics uses only keywords, such as `vibrant colors`, `close-up`, `black cat`. It is also evident that the model did not consider all the listed words.

The general recommendation for Midjourney suggests that prompts should be short and contain minimal information describing the expected result. However, a review of the publicly available user gallery suggests this is not always true, and experimenting is much better. No one said we can't combine different strategies either.

Below, we see a simple visualization of green smoke on a black background, generated by the `niji` model, characterized by the anime style. The prompt itself consists of only a few words, but apparently, it's hard to talk about particular matching to my needs.

At this stage, details weren't significant. I was more interested in obtaining a general style that would serve as a reference for further graphics. Here, using the model's help, I translated a simplified description into a precise vision of what I wanted to achieve.

Thus, the first version visualizing avatars in a style I liked was created. This is a good foundation for generating subsequent graphics. Moreover, the prompt I used to create the first version serves as a meta prompt. This means we can conveniently use it to generate more avatars.

Specifically, the prompt contains constant elements but also offers the possibility to replace a part describing the character itself. This is visible in the screenshot below, where I marked it with a placeholder `[DESC]`. Here again, I had a general idea of the characters I needed but used the model's help to generate a richer keyword description. Besides, I also attached an image presenting the first version of the avatars.

The result of the first iteration is visible below, and further attempts will depend on our individual preferences. However, we see that a consistent style has been maintained for each of the characters, excluding single graphics that go beyond the color palette.

But that's not all, as the developed style can be connected with existing graphics or even photos. In most popular generative graphics tools, we have the option to attach a character that the model will replicate. As exemplified by `Alice`, the results are adequate enough that we can choose the one that suits us best.

Although I chose an illustration style in this case, nothing stops us from selecting others, including very realistic ones.

Midjourney, through which I generated the graphics above, is characterized by excellent quality but has very low controllability. However, the biggest problem is the lack of an official API (unofficial wrappers may result in account blocking). Despite this, **all the above techniques** can be applied in combination with ComfyUI or similar tools. By this, I mean primarily: **styling, meta prompts, and reference images**. An example of this is the workflow below, which illustrates this in practice. Of course, due to the usage of a different model, the effect differs from the previous one, but the scheme remains the same.

## Graphics Generation Based on Templates

As visible, ComfyUI offers immense capabilities for influencing the shape of generated graphics. Nonetheless, this might turn out insufficient in practice, especially when we aim for very specific templates.

Here, solutions like [htmlcsstoimage](https://htmlcsstoimage.com) come to aid, allowing for graphics generation via API based on HTML templates. We thus have the ability to dynamically swap texts and graphics or even CSS styles. But now, two additional elements come into play — Vision Language Models and Generative Graphics.

We can:

- Define an HTML template containing specific color schemes, fonts, and a general layout, including responsiveness and behavior adjustment depending on the size of elements (e.g., text amount)
- Build a ComfyUI workflow and/or prompts for other tools generating graphics or template elements through API
- Create an app that reacts to events like adding a social media post, blog entry, or newsletter to the queue, and then generates a series of graphics and forwards them for verification
Such a scheme (excluding generated graphics, which were not available at the time) was used for nearly 3 years at eduweb.pl, thus generating promotional materials for 5 different editions of the newsletter. The graphics were tailored to the author or author and the specialization and content of the newsletter itself.

![](https://cloud.overment.com/2024-09-27/aidevs3_templates-77a3823f-1.png)

Despite the responsiveness, the above templates were quite static due to the need to limit user intervention. Now we can afford much more, as some errors can be automatically fixed by the model.  
## Summary

Tools in the area of generative graphics allow for the creation of automation and specialized tools capable of transforming images according to a defined style. For this reason, it is worth getting interested in them, even if we are not directly involved in processes related to graphics.

Among all the solutions mentioned today, ComfyUI and [HTMLCSStoImage](https://htmlcsstoimage.com/) deserve special attention, as they intersect strongly with the programming area. With their help, we can build advanced solutions capable of supporting or even automating elements of the marketing or product process. Ultimately, the value of knowledge about building programming solutions that use generative graphics models is sufficiently illustrated by the examples of Pieter Levels' products from the beginning of this lesson. Other applications can also be found online, directly related to the design process, creating advertisements, or photo editing.

**IMPORTANT:** If you do not have a computer that allows you to work easily with ComfyUI and do not want to use paid tools, you can skip the video below.

<div style="padding:75% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1029104946?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="02_03_comfy"></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

Good luck!