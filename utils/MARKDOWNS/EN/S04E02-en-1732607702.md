![](https://cloud.overment.com/S04E02-1732553638.png)

We have already gone through at least a few examples of operations related to document processing, loading various data formats, and content transformation. We have also examined their organization in a database and both full-text and semantic search.

In today's lesson, we will combine everything into a whole, introducing some improvements in prompts and their evaluation along the way. Our main goal will be to create an interface that, through relatively simple functions, will enable **loading a document**, its **summary**, **answering questions based on the content**, **translation**, and **information extraction**. The reason why we want to do this is the AI agents whose skills we want to simplify as much as possible.

As a result, it will be possible to delegate the following tasks:

- Access the page `https://...` and list all the mentioned tools
- Download this document (`link to a DOCX file`), then prepare its summary
- Translate `this document` from Polish to English
- Based on the files `x, y, z` answer the questions: `a, b, c`

An example of implementing the first point in code is visible in the screenshot below. We use the `process` function, which we built in the lesson S03E04 — Data Sources, allowing us to access the contents of the given web pages in the form of a list of documents. Then, the retrieved content goes to a new `extract` function, allowing us to pass a description of the content that LLM has to extract for us.

![](https://cloud.overment.com/2024-10-17/aidevs3_extract-be1d72f6-a.png)

Such an interface will allow us to build an agent in future lessons that will have a list of skills (functions) at its disposal. It will be able to call them in any order with specified parameters. A complete list of these skills can be found in the example `docs` and the `DocumentService.ts` class.

![](https://cloud.overment.com/2024-10-17/aidevs3_actions-48bfcd39-1.png)

The list of available methods currently includes typical actions for text processing. However, there is nothing to prevent expanding it with additional skills, such as:

- Document correction: correcting typos with possible style corrections and increased readability
- Document verification: checking correctness based on the model's base knowledge and internet search results
- Structuring the document: this can be the action discussed in the example `notes`, responsible for structuring voice notes
- ...and others.

The key in this puzzle, however, is that subsequent skills are built by **using existing components** and tool sets. Therefore, we no longer need to create everything from scratch, and additionally, we maintain control over individual components.

## Goals and Applications of Document Processing

Document processing is associated with business processes, but in our case, this issue is much broader, as it concerns most actions that agents will undertake. A few examples:

- Searching the Internet and working with website content
- Classifying messages, applications, files
- Combining information from different sources
- Listening to voice messages and videos
- Processing user conversation history
- Organizing 'memories' in long-term memory

Thus, we can say that practically everything an agent works on will be some form of a document. Meanwhile, **all actions related to their processing** will take the form of a **tool** with a structure similar to the example `todo` discussed in the previous lesson.

Before we create them, however, let's take a closer look at how the individual actions are built.

## Loading a Document into a Database and Indexing

Due to the growing context window limit and prompt cache, we will more often have the comfort of loading entire document contents into a system instruction. However, this will not always be possible due to the precision of the query or working with open-source models. In such cases, we will want to divide our contents into smaller fragments and add them to the Qdrant and Algolia indexes for later search using the hybrid search discussed in lesson S03E03 — Hybrid Search.

From our interface's perspective, it only takes loading documents using the `process` function (dividing the content into 1000 tokens) and executing the `answer` function.

![](https://cloud.overment.com/2024-10-17/aidevs3_answer-fd9bbe4b-2.png)

The function `process` in this case:

- **Recognizes the type of source:** in this case, it is a link to a file whose content must be downloaded to disk.
- **Divides the text into smaller fragments:** specifically using tiktokenizer to divide it into parts not exceeding 1000 tokens, describing each with metadata
- **Returns a list of documents**: the result of its action is an array of objects describing the individual fragments

We then execute a loop that saves the documents in the database. Additionally, we set the parameter responsible for adding the documents to search engine indexes to `true`. An example of one of them visible in Algolia is below.

![](https://cloud.overment.com/2024-10-17/aidevs3_index-80290d72-b.png)

The prepared documents then go to the `answer` function in which queries optimized for two types of searches are generated. Interestingly, if the user query is complex, the prompt `queriesPrompt` will generate a **series of sub-queries**.

![](https://cloud.overment.com/2024-10-17/aidevs3_hybrid_queries-8543f038-9.png)

All query pairs are then used to perform **parallel** queries **with a filter limiting the search scope to documents sharing `source_uuid`**. Their results are aggregated based on the `RRF` ranking and combined into a context for the model's response.

![](https://cloud.overment.com/2024-10-17/aidevs3_hybrid_search-386e7d67-8.png)

Therefore, we have a tool with which we can "answer questions" on practically any content provided by the user. We just have to keep in mind that this type of search will only allow us to **find information contained in the content**, not to obtain a **general perspective** and analyze relationships in the document, as for that we would need a graph database.

## Processing Document Fragments

In lesson S03E01 — Documents, we talked about the token limit for **content generated by the model**, which is still relatively low. Therefore, for transformations that require **rewriting the entire document**, we need to process each fragment individually.

An example of such an action is the `translate` function, which also requires passing a list of documents whose number of tokens **does not exceed the output token limit** of the model we are working with. In my case, I still include a buffer, as for each document, the model also generates a series of "thoughts" on how to carry out the translation.

![](https://cloud.overment.com/2024-10-17/aidevs3_translation-af25933d-9.png)

Because the translation of individual fragments can occur independently, the process allows for parallel processing of five of them. However, a critical element of this process is ensuring that **the model always returns only the translated content**. Otherwise, when all fragments are combined, we end up with a document containing unwanted comments.

![](https://cloud.overment.com/2024-10-17/aidevs3_translate-715acac5-d.png)

We already know that we cannot be sure of the effectiveness of a prompt. However, we can increase the likelihood of a correct response through a series of tests that verify the prompt's operation. These tests must be based on model evaluation, which we can perform using the `llm-rubric` type in PromptFoo. Examples of several can be found in the file `docs/prompts/translate.ts` and run with the command `bun docs/prompts/translate.ts`.

![](https://cloud.overment.com/2024-10-17/aidevs3_evaluations-0b7b3f02-2.png)

Besides a response devoid of additional comments, we will also be interested in maintaining the original writing style, which we also address with the prompt.

## Efficiently Passing Long Content in Agent Logic

In lesson S03E04 — Data Sources, we discussed the `reference` example, which allowed the model to use long content using the placeholder `[[uuid]]`, which we programmatically replaced with the target content. Now in the `docs` example, we see how significant this technique is and why **we will always want to store the document's content in the database (even temporarily)**.

In this case, it will not only be about the situation where content is passed to the user in the final response but also between stages of its processing (e.g., translation → summary).

In the current `docs` example, passing the document list is done through code, but it will not always be that way, as sometimes LLM will have to decide which actions to initiate on the selected data set.

We don't use `uuid` without reason, whose example value looks like this: `601595c1-8646-4408-b2da-82522604a942`. From the model's perspective, this is a better format than a numerical `id` because if there is a mistake in the record, we can effectively detect it. Unfortunately, there is still room for switching the entire identifier, but here again, the rule "verification is simpler than generation" comes into play, i.e., using a prompt that verifies the decision made. Again, we can't be sure it won't fail, but the risk of this happening drops significantly.

## Summary

Today's lesson showed us what we have been striving for for a long time. It is about the concept of building a **set of tools** that can perform individual tasks but that we will be able to **combine in various configurations**. Moreover, this possibility can, in part, be delegated to the model, opening up space for building specialized AI agents.

Contrary to the `todo` example from the previous lesson, here instead of connecting to an external API, we rely solely on the capabilities of the language model and a set of actions built by us. Nevertheless, we can still build a tool for the model based on them, just as we did in the case of Todoist.

Therefore, if you are to take only one thing from this lesson, try to consider (or better yet, build) the logic that enables communication in natural language with the actions we discussed today.
Good luck!