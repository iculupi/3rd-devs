![](https://cloud.overment.com/S03E04-1732097139.png)

We already know quite a bit about content organization, its searching, and delivery to the model. It is now time to focus a little on data sources and various file formats whose content is not always accessible to the model.

Even though multimodality already allows us quite freely to process images, sounds, or even videos, we still face various limits that require actions on the code side. Conversely, with other file formats like pdf, xls, doc, or zip, the situation becomes complicated.

Today, tools like LangChain or CrewAI strive to offer us easy loading of various file formats. However, it quickly turns out that the default 'loaders' lack details crucial for such critical elements as outlining the context for the model. I mean metadata here, which we discussed in lesson S03E01 — Documents and which we must take care of ourselves.

## Working with files and directories

The content of files loaded into the LLM context should also be stored on a disk (the file itself) and in a database (information about the file). The reason is not only the need to use this data in the future but also the processing itself, which may consist of many steps.

Therefore, depending on the programming language you are working with, it's worth familiarizing yourself with the available API allowing interaction with the file system. Besides, it's worth remembering a few details:

- Uploaded files should be categorized
- The directory structure should consider the creation date so that we do not end up with tens of thousands of files within one folder
- The directory structure should include the file's "uuid", which will help avoid accidental overwriting of an existing document
- It's worth preserving the original file name both on the disk and in the data saved in the database
- The content of the files should be verified based on `mime-type`, not just the name (unless it’s a text file, e.g., markdown)
- Temporary files should be deleted as soon as they are no longer needed

In the database, information about the files can come directly from the generated document, whose example we see below.

This document describes the content of an `xlsx` file, which, apart from textual content (which has limited formatting due to necessary simplifications), also contains a `screenshots` property with a list of screenshots of individual sheets. This way, the model can 'see' complex elements such as charts or table structures. Below is an example of a sheet screenshot showing a simple data visualization.

Meanwhile, we will return to the benefits of storing files on a disk and in a database later.

## Loading the file content

Reading the contents of a plain/text file, from a programmer's perspective, is straightforward. Things become slightly more complicated with images and audio files, but here, due to LLM capabilities, this can also be considered relatively simple.

Problems begin to appear with large files, a large number of files, or when these issues combine. Major problems also occur with formats like docx, xlsx, or PDF. The ability to access content and transform it varies depending on the programming language. However, depending on the situation, we can approach this task more creatively and utilize the fact that someone put a lot of effort into handling different types of documents.

On the web, we can find many tools and libraries offering the capability to change the format from `xlsx -> csv` or `docx -> HTML`. Most do not handle even quite simple structures very well, albeit it is worth checking them since they might work in our case. Services offering an API perform much better, for whom correctly reading the file is an important business element. Google is one such company.

Therefore, I prepared an example `loader` capable of loading content from files of different formats through a simple `process` function, which is tasked with returning a list of documents (content + metadata).

Important: to run the project, and you must fill in the `.env` file with the keys `GOOGLE_PROJECT_ID`, `GOOGLE_PRIVATE_KEY_ID`, `GOOGLE_PRIVATE_KEY`, `GOOGLE_CLIENT_EMAIL`, `GOOGLE_CLIENT_ID`. Their values can be obtained after creating a project "[Google Cloud](https://console.cloud.google.com/)" where you must enable the Google Drive, Google Sheets, and Google Docs APIs. Then, in the [IAM & Admin console](https://console.cloud.google.com/iam-admin), download the keys for the IAM Account in JSON format (Manage Keys -> Add Keys -> JSON -> Create).

Let's start, however, with the fact that I've based this example on tools I use myself, although in this new release I have not yet had the opportunity to test their effectiveness on a larger scale. Therefore it's highly likely that this code still contains some errors and can be optimized in various ways.

Let’s focus on its functionality, which presents itself as follows:

As you can see, we support four types of files: text files (e.g., markdown), documents (docx, xlsx), PDF files, images, and audio files. Specifically, it breaks down as follows:

- **Text files:** due to their simple structure, they are simply loaded and converted into a document, which can possibly be split into fragments
- **Documents:** docx/xlsx files are first uploaded to Google Drive, then fetched in HTML or CSV format and only then to markdown. Additionally, I also download them in PDF format, whose individual pages I convert to JPG files (screenshots)
- **PDF:** this format causes the most trouble, and I do not expect it to handle complex structures. However, I occasionally read invoices or bills, so I take its screenshots and (if possible) also download its content
- **Audio:** first, I detect the overall silence level, then segment the entire document into "silence segments" and segments during which someone speaks. Next, I split the entire file into pieces, considering a small buffer at the beginning and end of each segment. The files are then converted to `ogg` format (very optimal) and transcribed by OpenAI Whisper.
- **Images:** here, I have the least possibilities, and loading documents involves merely processing them through VLM, whose response is converted into a textual form

Ultimately, we get an answer in the form of a list of documents ready for loading into the context and database.

Therefore, to use the `loader` example, it is enough to pass the file path or URL. If only its format matches one of the supported, we will receive the correct content. By supported formats, I mean:

- Text files: .txt, .md, .json, .html, .csv
- Audio files: .mp3, .mp4, .wav, .ogg
- Image files: .jpg, .jpeg, .png, .gif, .bmp, .webp
- Documents: .pdf, .doc, .docx, .xls, .xlsx
## Proxy for external API

Data for LLM will not always come from a file. Often, the source will be an external service (as in the `linear` example), e.g., Notion or any other API providing some kind of information.

Although theoretically, we could connect directly with the external service, we almost always will want to build our "proxy" which will mediate between our agent and the service.

The reason is greater flexibility and the ability to adjust both **response format, error format** or **combine information from different tools**.

The idea of a proxy is not particularly complicated, but I bring attention to it because it plays an important role in planning the tool structure for the AI agent. In practice, we even have such a proxy in the `loader` example, where Google Drive's response is not immediately loaded into the model's context but first converted into a data structure consistent for all other tools, namely documents (I mean "content + metadata").
## Website content

Through the lens of "documents", we can also look at the content of websites. Therefore, we can use tools like FireCrawl (or alternatives) and combine them with the logic we have in the `loader` example.

Thanks to this, we can send any URL from permitted domains to the `process` method. Then the page content will be converted into a document format and ready to work with LLM, as well as saved in the form of a `markdown` file.

In this area, there’s nothing new beyond what we discussed in the `websearch` example and the lesson S01E01 — Interaction.

## Remembering user-provided information

The source of information can also be the interaction with LLM itself, including data coming from the user and model. The thing is, such interaction will usually be multi-level, so "rewriting" content each time will not be advisable. Instead, we can equip the model with the ability to save the content of the file so that in further steps it only uses the identifier.

The `reference` example shows a simple scenario in which the model has information about a longer document with the instruction that in case of needing to cite its content, it should use the placeholder `[[uuid]]`. Then we programmatically replace the content that can be passed to the next prompt or returned to the user.

A practical situation in which we might want to use such a scenario is an agent sending an email. Below we see the content of one such message in which Alice sent me a summary of one of the YouTube videos.

Summarizing this:

- The model should have the capability of reading and writing files
- Apart from file writing, the model should be able to share the file and get its URL for further work
- The model should be able to use identifiers/placeholders, thanks to which it will not be necessary to rewrite lengthy content (which often will also be impossible due to output token limits).
- Practically **every kind of data** the model works on should be (this is a suggestion, not a necessity) reduced to one format, common to all kinds of content.

## Organizing data in a database

Saving document data in a database can take various forms, depending on how we will use them. The most likely scenarios include:

- Linking the document to the user with restricted access to its content for other users
- Linking the document to the conversation, enabling context recall in case of resuming the conversation
- Linking the document to the task carried out by the agent and/or stage of the larger process, in case it needs to be resumed
- Storing fragments of processed content. E.g., when translating a long document, we save completed fragments to resume the process where the problem occurred
- Saving the original content of the document and its modified forms in case the need arises to refer back to it or repeat the processing process
- Recording the document's expiration date information, especially for files shared under a public URL
- Linking the database entry with the content of the file located on the disk. For shorter documents (e.g., generated chunks), it is advisable to store their content directly in the database.

The topic of storing content in a database will appear in further lessons of AI_devs 3.  
## Working with Sensitive Data

We already know that when working with files containing private data, we can use models operating locally or take advantage of business plans of services like Amazon Bedrock, as long as it is permitted by company regulations and client agreements.

However, processing data by a model is not the only situation we should remember because if we hand over the content of files to a model, and the related permissions are too extensive, we risk a data leak.

For instance, if an LLM is responsible for saving the file and assigning it to a user, there could be a mistake. In such a situation, **it is necessary to control this process on the programming side** and save the file based on the active session or user token, independent of the model's output.

The same applies to situations where the model decides to send an email or use a communicator. Here, programming restrictions are also necessary.

Essentially, whenever there's room for error when processing files, we need to involve either code or ultimately a human. For example, the mentioned emails can be created as "drafts," which have to be approved by the responsible person.

Another example of a restriction we have been using is limiting the domains that could be searched or scraped. Although our main concern was maintaining content quality, the issue of privacy is also very important here.

The golden rule is that **the model's contact with the external world should be limited as much as possible** through programming restrictions.  
## Summary

Today's lesson showed us that LLMs can be connected to practically any data source, although it requires significant coding effort.

We have also once again realized that imposing our own limitations on the system is critical from both the stability and privacy perspectives.

Ultimately, it will depend on us how much freedom we allow models and whether we will verify the effects of their work. We must keep in mind that we are responsible for the system's functioning, our company, or our clients. It is worth approaching this topic carefully.

I also think that what we saw in this lesson regarding uploading files to context nicely connects to everything we've learned so far. That's why it's important to test the example `loader` operation and either familiarize oneself with its content for further development or create a simpler version more tailored to us.

Essentially, a tool enabling the upload and saving of external data sources to the model will be indispensable in the coming weeks of training.