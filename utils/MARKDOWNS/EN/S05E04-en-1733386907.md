![](https://cloud.overment.com/S05E04-1732627999.png)

I have repeatedly mentioned that it is worth designing AI agents with a clearly defined area in mind. Additionally, their contact with the outside world should be limited to eliminate some problems "by design."

The truth is that the agent will need to interact with the world, and possibly even with users. However, this won't always take the form of chat or voice conversations, as no one says these are the only forms of interaction we have at our disposal.

What I'm getting at is that **it's worth approaching both the construction of agents and managing their access to information and task lists sensibly, but also boldly.** For this purpose, everything we have learned so far about the models themselves, as well as the design principles I have suggested, will prove helpful.
## Interaction with users

We've found many times that controlling the behavior of the model requires precision, and we cannot rely on reading minds. The problem is that users unaware of LLM (Large Language Model) limitations still have such expectations, translating into how they use the tools we create and ultimately— their experiences.

Moreover, it is currently difficult to create an agent capable of performing every task. Especially since there are those that can challenge even the best system.

And seriously, in such a situation, the system should inform the user about the lack of available skills and any available options. The problem is that in the case of a chat interface, we cannot predict all possible queries. We also know that due to the vulnerability of models to prompt injection, it's challenging to talk about full security against both attacks and ordinary mistakes.

Of course, the response above should look something like this: 

Another issue is the very mode of communication, which is decidedly different in the case of someone knowledgeable about the agent's capabilities. An example of this is visible below because my request refers to the available integrations with **Linear**, **Google Calendar**, **CoinMarketCap**, or **Resend** even though my request started with "send an email" and then listed the individual activities, the agent correctly established the order of these tasks. So the system's flexibility is high, but it is certainly not **unlimited**.

Furthermore, for complex tasks, there may be problems with external services. But if actions related to them are not critical, they should not disrupt the execution of the remaining part of the task. This is evident below, as Alice informed me that she was unable to fetch events from my calendar in this case.

The above situation is a direct result of what I said about returning **documents** (text + metadata object) as a result of tool operation. In this case, "error handling" involved informing the agent about its occurrence.

However, this does not mean that error handling itself solves all problems and every user will be able to work with an agent like Alice. The reason is that some commands must be issued very precisely to support the agent's reasoning. For example, a request to visit selected websites and prepare a summary, in my case, looks like this:

- Read the content of the pages `*addresses here*`, then create a document containing `*description of expected content here*`, upload it to the server and send it to me via email

You can see here how my command **guides the agent** through a scheme involving a series of steps needed to complete the task. In contrast, a typical user would rather say:

- Fetch `*general description of expected content*` from the pages `*list of addresses*` and send it to me via email

Such a query is less precise and, depending on the situation, may negatively affect the agent's performance. Worse, even if we address this specific case, we still cannot predict all possible queries.

But then, **how to make it so others can use the agent?**

There are at least a few options:

- The agent can be limited by an interface that does not allow too much interaction from the user. An example of such an agent is Perplexity. Depending on the query, various steps are taken to increase precision and deliver a high-quality result. The user has no significant influence on this process, nor the possibility to request an action unrelated to searching, as the interface limits them.

- The interface itself can further limit the user's freedom (however it sounds), yet still perform valuable work for them. Here's an example from one of the previous lessons: automation connected to my email inbox and messages with a specific label. Based on the information from the email, a message is created for the agent, who in this case adds a new task in Linear. We're still talking about value for the user, while limiting the problem of imprecise messages.

On the other hand, there may be situations where we do not want to limit interaction with the user. In that case, we must ensure good practices for production applications, which we discussed in the first part of AI_devs. Additionally, onboarding becomes necessary, explaining to users how they can work with the agent and what expectations they may have of our system. An example could be my application heyalice.app, which allows for **customizing its settings** to fit one's preferences. This, of course, creates a huge barrier to entry for new users but also provides a lot of freedom to those who go through the configuration stage.

I have already said that, in my opinion, currently, the most complex agents should operate "in the background." Meanwhile, those who have contact with the user should be maximally limited already at the project assumption stage. Of course, you can disagree with me here, especially since the development of tools and models themselves sooner or later may make such an attitude outdated.

Going further, more advanced agent logic will prevent real-time interaction or low response time interaction. The thing is, not all interactions with the agent will require performing advanced "reasoning," so we can use the "router" concept, which I've started to use recently.

It involves simple query classification to determine whether any tools and memory areas are needed or not. If the router decides there is no need, we simply generate the response.

Although there's a catch, because sometimes the line between needing a tool and providing a direct answer is very thin, even for humans. It's worth being cautious here or simply remembering to emphasize the necessity of using tools or "pondering."

Regarding waiting for a response, in lesson S00E03, I also mentioned practical agent action streaming, and here I wanted to add one of the latest updates to Vercel AI SDK, which facilitates this process. Thanks to this, the server can inform the user about the action being taken without waiting for the entire query to be completed.

So to summarize the topic of agent interaction with humans— **universal systems capable of fully autonomous operation are still beyond our reach.** Therefore, we should focus on **specializing agents** in a selected area and imposing limitations on the logic and interface side, so that the room for error or undesirable interaction is as small as possible.
## Collaboration with other agents

Although there is a lot of information online about building teams of agents capable of solving very complex problems, my experience suggests that it is first worth focusing on the ability to create **one agent** that does its job well. Then connecting it with other agents becomes much easier, but sometimes completely unnecessary.

In AI_devs 3, we indirectly discussed solutions that could just as well be compared to systems built from many agents because:

- Tools connected to the model took the form of **independent modules** and maintained a consistent interface. However, their logic often included comprehensive interaction with LLM and sometimes with each other. This means that, for example, the `websearch` example could be developed into full agent logic, and then our main agent would contact it regarding Internet searching.
- In our case, **task and action planning** also occurs through several model queries, but we could further develop this stage into a fully-fledged agent if needed.
- Tools have access to **shared state** and **local state**, allowing them to exchange information among themselves, much like in the case of many agents.

So we continue to meet the main assumptions regarding **independence** and **consistent data exchange format**, and we can only develop a given module when there is a need. Then another agent simply becomes another tool that can be called when needed.

On the other hand, agents can have a much simpler structure than what we built in AI_devs. A good example is the mentioned [Ditto](https://github.com/yoheinakajima/ditto), which uses only one prompt and a set of a few simple tools activated with Function Calling. The prompt below is very simple compared to those we discussed. Despite this, Ditto can build simple applications within a single thread.

Both in the case of Ditto and the context of our lessons, we still talk about one agent capable of performing more or less complex tasks. However, sooner or later, we will want to take a step further and find a real reason to build and connect many agents. Currently, however, this is a topic of exploration, and it's difficult to clearly identify proven work techniques and patterns.

However, publications such as [Scaling Large-Language-Model-based Multi-Agent Collaboration](https://arxiv.org/pdf/2406.07155) are emerging, presenting proposals related to the architecture of such systems. This concerns, therefore, the connection of agents in a chain, tree structures, or various types of graphs.

Viewing agents in this way opens directly absurd possibilities, an example of which is [Project Sid](https://github.com/altera-al/project-sid), assuming the creation of a civilization of agents. Of course, at this point, it is worth maintaining significant distance from this project, but the direction it outlines may inspire unconventional solutions to our problems.
A much more down-to-earth example is the entry [Orchestrating Agents: Routines and Handoffs](https://cookbook.openai.com/examples/orchestrating_agents) shared by OpenAI. It shows an example of implementing a team of agents responsible for customer service. Here the difference from what we learned is much smaller, and already now, creating such a system should not pose a significant problem (although everything boils down to details).

The conclusions from these examples are as follows:

- Where not necessary, agency logic, heavily relying on model action, should be omitted. Using a simple "chain" of queries to an LLM will often be fully sufficient for simple tasks.
- One agent can be equipped with many tools and use them successfully, adjusting its action plan to changing conditions. However, there are limits. At some point, tools may start to overlap (but differ in details), and the system will not work correctly.
- We can then create a team of agents and decide when to activate them. This way, we gain the ability to further scale the system and increase its capability to solve complex tasks.

## Agent API

Some of the examples we went through allowed for interaction via the `/api/chat` endpoint, where we could usually send a query written in natural language, and the model decided what to do with it. And it will look exactly this way in the case of an agent or a team of agents. I call this "single-entry point." Such an endpoint gives us great flexibility, but we will not need it in every situation.

Let's assume our agent has information about the environment: location, weather, and several other data, potentially useful for performing its tasks.

These data are characterized by the fact that they change over time, and, additionally, their real-time retrieval will not always be justified. For example, in my case, the information whether I am in a car is updated when my phone connects to it via Bluetooth. The entry update itself does not require agent logic engagement, so I can directly query the address `/api/users/:id`.

Similarly, with access to tools. My agent has the ability to upload files to the server. But often, I want to do it myself or take advantage of this possibility through automation. In this case, again, a simple query to the address `/api/files` is enough. Therefore, the API structure can be as follows:

The concept of the API itself is not something new in the context of applications. We usually use it to allow various interfaces (e.g., mobile, web, and desktop applications) to communicate with the same server. It's similar with the agent, only here we talk about external services that will provide information to the agent without involving its main logic. And then this "main endpoint" comes into play, capable of processing natural language queries.

Ultimately, the API structure will depend on the application and how it interacts with it. However, we must undoubtedly keep in mind the variable of **the presence of LLM**, which affects the architecture precisely because one of the API users can also be other agents.

## Connecting with Various Interfaces

In the entry "[How I Work with AI](https://bravecourses.circle.so/c/dyskusje-ai3/jak-pracuje-z-ai-material-bonusowy)," I talk about using a **shared back-end** for several interfaces in the form of applications and devices. Although the logic operating on the server remains the same, there are a few things worth noting.

First and foremost, messages are tied to a conversation via an identifier. In turn, conversations are tied to the user. From a programming perspective, it happens exactly like in classic applications.

Such data organization allows connection with various interfaces, which can only send **the latest user message** and **the conversation identifier**. An example is my Shortcuts macro launched on the phone or watch, where **before sending the query**, I query the server for **the identifier of the last conversation** that took place within the last 15 minutes. Thanks to this, even though at that moment I only send the current message, the conversation still takes place within the given thread.

However, there is a variable that distinguishes whether I communicate with the back-end from a watch (audio interface) or a desktop application (chat interface). Specifically, apart from the list of messages, I send an additional property specifying **the type of interface**. This information is used in **the prompt responsible for generating the final response** and tailoring it to the given format. For instance, a message displayed in a chat window will look different than in the case of an audio form.

In some situations, we can also decide that the conversation identifier comes from an external application. An example here is Slack, where we have available **threads**. What's more, the message formatting syntax in the case of Slack is also quite characteristic, and using the **source** property here makes sense.

Ultimately, we must decide which interface will be best for our agent(s). However, it is worth considering using already existing solutions instead of building something from scratch (unless, for some reason, this is important to us). Alternatively, we may decide that our agent will not have a user-accessible interface at all, and that is also one of the options.

## Real-time Operating Agents

The possibility of obtaining an immediate response from an AI agent is an expectation from many individuals and clients. However, practice shows that an agent usually needs more time to respond, and often the only form of optimization will be streaming information about his progress.

This does not mean, however, that logic involving LLM and tools must always eliminate real-time interactions. It shows the example of `live`, which differs from the example `tool_use` only in that I switched the model from GPT-4o to Llama 3 from the [Groq](https://groq.com) platform. Thanks to this, the reaction time is "only" 856 seconds, BUT in this case, there is no actual tool execution. So one might say that the response time will increase to 1 second, but that is still a drastic change compared to working with OpenAI.

At the same time, it should be remembered that at the time of writing these words, platforms like Groq or Rysana still do not offer sensible pricing plans where API limits would allow for practical application of their services (according to creators, however, it is possible to switch to a business plan, but I have no information about the conditions and the minimum amount).

If, however, we want to stay with OpenAI/Anthropic and use their models to build such an interface, then:

- The role of LLM should be limited to the necessary minimum.
- Where possible, we must optimize prompts so they can work with GPT-4o-mini or Claude 3.5 Haiku models.
- In situations where audio processing is required, we can skip the transcription and audio generation step using external services, using the latest capabilities of OpenAI models, which handle audio both in input and output.
- We should use streaming wherever possible, including audio streaming.

However, this still concerns scenarios utilizing relatively simple tool interactions. **Wherever more complex logic is required, real-time interaction can, for now, be set aside**.

## Agents as Tools

All the time, we considered agents as systems capable of using tools. However, the agent itself can also be one of these tools or even be responsible for **only one stage of logic**.

For example, [Weaviate](https://weaviate.io/blog/what-is-agentic-rag) describes a so-called "Agentic RAG" example on its blog, which, as the name suggests, is an agent responsible for loading content into the LLM context.

In the same way, we can view agents in the context of other tools, e.g., browsing the Internet or scraping website content.

Essentially, wherever there is a problem requiring dynamic adjustment to the current situation, it is worth considering creating an agent that will solve it. Subsequently, other agents may only turn to an agent specializing in a given task, sending him queries written in natural language.

Building such an agent is no different from what we have discussed so far and what we will focus on tomorrow. Just the scope of its operation will be significantly smaller.

## Knowledge-gaining Agent

While one can encounter many questions about ways to connect an agent with own knowledge online, almost no one asks how an agent can gain knowledge. Moreover, much suggests that this approach is far more effective than attempting to connect an LLM with documents created for humans.

We have repeatedly convinced ourselves how difficult it is to process some document formats. Anyway, even if we manage, there is still a high risk of losing context at the stage of searching and loading content into the system prompt.

However, the situation changes when the agent's memory ceases to have the form of content loaded from external sources and instead can build it according to its own rules. Here, an example that made the biggest impression on me is the [AGI Memory System](https://github.com/cognitivecomputations/agi_memory) repository, which describes table structures and ways of interacting with them in the context of dynamically built memory. The whole covers practically everything we've learned so far and organizes issues related to full-text searching, semantic searching, or graph work. However, in this case, everything is fully based on PostgreSQL, where we store both embeddings and a simplified structure of connections between memories.

The difference between memory built by loading document content and constructing memories based on them is fundamental. The main difference here is that, in the latter case, the agent **independently determines the information organization method**, allowing for maintaining consistency even when data comes from different sources. Such consistency allows for much more effective searching and further expansion.

My experience suggests, however, that fully dynamic memory-building systems are still beyond our (or my) reach, which relates either to the capabilities of the models themselves, the tool ecosystem, or the techniques of working with them. Therefore, it's worth imposing restrictions, even in terms of **category tree** or **tag clouds** within which the model can organize information. We have an example of initial structure below.
And now, using what we know about search engines and vector databases, it suggests that:

- recording new information will require extracting relevant data from it, often breaking it down into smaller documents. Then, this data will be classified and assigned to specific categories. Ultimately, each document will also be appropriately described to enable filtering at the search stage
- reading information will involve generating queries against the above structure and then conducting searches that collect all the required memories.

However, the challenge in this case is to avoid situations where some information is saved in category "A," and later we attempt to find it in category "B." Thus, we can perform two types of searches—narrow (restricted to a category) and broad (scanning the entire available knowledge base). As a result, the risk of the mentioned error will be lower, but records from broad searches will need to be filtered by the model itself (here, re-ranking performed by LLM comes into play).

## Summary

During AI_devs, we went through dozens of tool examples and learned how to create them with AI agents in mind. Finally, we also learned the foundations that allow building logic where an LLM almost independently uses these tools.

Such systems are capable of performing tasks whose method of execution does not have to be predetermined. Despite this, **we are not yet at the point where it is possible to build fully autonomous systems capable of executing entire processes.**

However, today nothing stands in the way of building systems capable of navigating a limited area, with a range of limits imposed by us. Moreover, these systems are slowly starting to collaborate, thereby increasing their capabilities each month.

At this stage, it is crucial to gain practical skills related to specialized agents and implementing them for private or internal company needs. Meanwhile, much indicates that libraries, frameworks, and work techniques will continue to evolve, helping to create increasingly complex systems that may be able to develop independently at some point.

Today, however, everything is still in our hands.