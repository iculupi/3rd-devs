![](https://cloud.overment.com/2024-11-05/s01e05-e0bd2fa2-4.png)

It's no secret that building tools for personal use, internal needs, or even the initial stage of an [MVP](https://en.wikipedia.org/wiki/Minimum_viable_product) differs from production applications. This difference is evident in the classic applications we develop daily. However, generative AI adds another element, increasing the difficulty level.

We've talked quite a bit about the limits of generative AI. We've also discussed possible solutions to common problems and continue to learn new techniques for working with models, even imposing our restrictions on them. Ultimately, we also know that some problems have no solution and can only be addressed by **deciding not to use language models** or by changing project assumptions.

In this lesson, we'll discuss working in a production environment, going through the process of deploying an application on your server. This process is crucial as it allows us to provide our API, which external services can interact with.

**Important:** If you have extensive experience working on the backend, you can set up your API on a chosen VPS independently. Also, pay attention to issues related to LangFuse and Qdrant.
## Local and Production Environment

Since we communicate with LLM through an API, deploying the application on a production server is quite a standard procedure. We need a VPS (e.g., [Mikr.us](https://mikr.us/) or DigitalOcean), a domain, an HTTP server (e.g., [nginx](https://www.digitalocean.com/community/tutorials/how-to-install-nginx-on-ubuntu-22-04)), an HTTPS certificate (e.g., [Let's Encrypt](https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-22-04)), a database (e.g., SQLite or PostgreSQL), and a vector database (e.g., Qdrant). Additionally, to optimize the deployment process, we can use [GitHub Actions](https://github.com/features/actions). This is a typical toolset that I use in production.

The application running in the local/development environment should replicate the production environment as closely as possible, including tool versions, dependencies, and database structure — this should be clear to everyone.

Differences between the local and production environments arise in areas such as **user data**, **API connections (e.g., "sandbox" modes or separate accounts)**, and now also the **prompts themselves**. This means there may be inconsistencies in the three areas **related to data processing**.

![](https://cloud.overment.com/2024-09-16/aidevs3_prodlocal-6514d51a-1.png)

We've long been addressing the issue of user data through synthetic data, automatically filled in the process of `seeding` the database. Sometimes this process is skipped or not given much attention. However, when data is processed by code, it doesn't always pose a big problem. However, with language models, the situation is different. Therefore, it is necessary to ensure that **data in the local database replicates production data as closely as possible** (while considering anonymization).

External services, with which our application communicates through the API, are also a source of data. Examples include tools such as CMS, CRM, task management, email, or document applications. In the development environment, we usually use separate accounts, which may also contain different data than production ones. Similar to databases, we must ensure that the differences are minimized as much as possible. Here, the solution is **to ensure configuration consistency**. For example, if we're creating a tool to support order management in a sales system, the test account must contain exactly the same categories that appear on the main account.

While all this may sound obvious, from my own experience, I know that few projects address these topics from start to finish. Therefore, I highlight this because it involves additional work, which significantly impacts the software development process.

Finally, we also have the issue of prompts, which are part of the application's source code, and their modification in a non-obvious way can affect the stability of the entire system. We already know that thanks to PromptFoo and LangFuse, we can develop prompts in a more controlled manner, which becomes critical, especially when working in a team. However, there is also a need to develop a set of test data sets, manage prompt versions, model versions, and the associated costs.

In lesson S01E03 — Limits, we talked about the limits of API platforms providing large language models. However, I did not mention the role of limits operating on our application's side. We realized their significance when one of our projects was queried 500,000 times. Thanks to [Cloudflare](https://www.cloudflare.com/) mechanisms, the losses incurred during this attack amounted to **~20 cents**. Without this, we would be talking about several, if not tens of thousands of dollars.

Everything I just said is best confirmed in practice. Therefore, we will now go through two scenarios that will allow us to publicly provide our API in two variants — an application hosted on our computer and a virtual private server (VPS). The first of these options is difficult to call production-ready, but it can work for personal needs, especially for people without backend experience and server management.
## Localhost Available on the Internet

In the `external` example, there is a simple application that allows conversation with a large language model but contains a few additional details that we have omitted so far.

First of all, in the `middlewares.ts` file, there is a `limiter` function responsible for imposing query limits. After exceeding **one query every 10 seconds**, the application will start returning the error below. This is a simple implementation of limits that should either be implemented by Cloudflare or by an expanded authentication mechanism for connecting with a given user.

![](https://cloud.overment.com/2024-09-16/aidevs3_429-7b2386c4-e.png)

Additionally, access to the application is blocked for connections that do not contain the `Authorization` header set to the value assigned to the `PERSONAL_API_KEY` key in the `.env` file. In other words, no one from outside will be able to use our application.

![](https://cloud.overment.com/2024-09-16/aidevs_401-de209274-6.png)

After starting the server with the `bun external` command, we can use tools like ngrok (free version), [localcan](https://www.localcan.com/) (7-day free trial), or self-configured port forwarding. For the former, sharing the application involves running a simple command `ngrok http localhost:3000` and then go to the address from the `Forwarding` line. Of course, for a free plan, this address will be regenerated after each launch, so it's worth considering purchasing a paid plan.

![](https://cloud.overment.com/2024-09-16/aidevs3_ngrok-5e61f19c-e.png)

## VPS Configuration

**IMPORTANT:** Configuring your server **is not required in AI_devs 3**, but we cover this topic because having remote access to the tools we design is very useful. You can skip this step and rely on the above-mentioned `ngrok`.

At the beginning of today's lesson, I linked DigitalOcean tutorials on server configuration, specifically on:

- [Nginx Configuration](https://www.digitalocean.com/community/tutorials/how-to-install-nginx-on-ubuntu-22-04)
- [Let's Encrypt Configuration](https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-22-04)
- [Node.js Configuration](https://www.digitalocean.com/community/tutorials/how-to-set-up-a-node-js-application-for-production-on-ubuntu-20-04)
- [PostgreSQL Configuration (optional)](https://www.digitalocean.com/community/tutorials/how-to-install-and-use-postgresql-on-ubuntu-20-04)

I use these entries when setting up my servers for development purposes. For production, I rely on support from people specializing in this area or others on the team handle it.

<div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1010135935?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="01_05_vps"></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

Summarizing the configuration shown in the video:

1. **VPS Access:** Purchase access to a VPS server: DigitalOcean or Mikr.us.
2. **New Server:** Create a new droplet (DigitalOcean) running Ubuntu with at least 2GB RAM (4GB recommended). Use an SSH key for authentication (you can generate one with the `ssh-keygen` command), and then copy the public key content from the file ending with `.pub`.
3. **Connection:** Connect to the server using the command `ssh root@server_ip_address -i ~/.ssh/private_key_name`.
4. **Nginx:** Install `nginx`.
5. **Server Block:** Create a server block in `/etc/nginx/sites-available` associated with your domain or subdomain.
6. **Server Block Activation:** Activate the server block with the command `sudo ln -s /etc/nginx/sites-available/your_domain /etc/nginx/sites-enabled/`, check the nginx configuration `nginx -t`, and if it checks out, reload it with the command `sudo service nginx reload`.
7. **Domain Configuration:** Add an A record to your domain, setting it to the server's IP address.
8. **Let's Encrypt:** Install `certbot` and generate a Let's Encrypt certificate for your domain.
9. **Node.js:** Install the latest version of Node.js and bun.
10. **Node.js Server:** Download [this example code](https://cloud.overment.com/aidevs3_vps-1726560176.zip) and place it on the server. Then fill out the `.env` file with your API key and any string of characters for the `PERSONAL_API_KEY` key.
11. **Install pm2:** `npm install pm2@latest -g` and start the Node.js server with the command `pm2 start bun -- start`.
12. Submit a query to the server:

> curl --request POST \
  --url https://yourdomain.com/api/chat \
  --header 'Authorization: Bearer PERSONAL API KEY' \
  --header 'Content-Type: application/json' \
  --data '{
	"messages": [
		{
			"content": "hey",
			"role": "user"
		}
	]
}'

Later in AI_devs 3, having your server where the application providing our private API runs can be very useful. Although it is not required for further learning, it is certainly a good idea to create your "production" work environment.

## Managing Prompts

Until now, we stored prompt content in the application's code. For simple instructions and the prototype stage, this may be sufficient. However, it quickly becomes apparent that making even the smallest changes, especially in a team, becomes a significant challenge. Moreover, the `memory` example discussed in the previous lesson well illustrates the difficulty of monitoring subsequent versions of prompts. At the same time, LangFuse (and similar tools like [Agenta.ai](https://agenta.ai/)) allows for managing prompts and their versioning.

In the `prompts` example within the `AssistantService` file, there's an `answer` method that, unlike what we did before, does not read the prompt from a file but fetches it directly from LangFuse.

![](https://cloud.overment.com/2024-09-17/aidevs3_prompts-eb680ed8-7.png)
The prompt itself is defined in the panel, within the "Prompts" tab. Here, we can define variables that are replaced with target values during the loading or compiling of instructions. After making changes, a new version is saved to which we can assign our own labels.

**NOTE:** To run the example, you must, of course, add a prompt named "Answer" to LangFuse with simple content like "As Alice, you're speaking to ...".

![](https://cloud.overment.com/2024-09-17/aidevs3_manageprompts-262c6fdf-4.png)

Managing prompts within LangFuse has a definitive advantage — monitoring. Each saved instruction can be linked with user interaction, facilitating later review of activities from the perspective of sessions, users, and the prompts themselves. Moreover, a prompt can be automatically verified through the 'Evaluation' functionality, where we can configure tests.

![](https://cloud.overment.com/2024-09-17/aidevs3_track-1799b624-e.png)

Automatic evaluation provides information about the quality of responses generated by the model, through assessment and short comments.

![](https://cloud.overment.com/2024-09-17/aidevs3_evaluate-5e2513c4-7.png)

The last thing to pay attention to at this stage is the inclusion of **user identifiers** and linking them to LangFuse logs. This allows for reviewing interactions, possible debugging, and insight into general statistics.

![](https://cloud.overment.com/2024-09-17/aidevs3_user-18d2f73c-9.png)

Adapting monitoring to your application will vary depending on the case. Definitely, it's a good idea to check out the interactive demo available [in the Langfuse documentation](https://langfuse.com/docs/demo).

![](https://cloud.overment.com/2024-09-17/aidevs3_demo-527a2980-a.png)

As for further configurations, in subsequent examples, we will aim for full monitoring (with some exceptions). Crucial will be developing the habit of defaulting to using monitoring tools (which don't have to be Langfuse).
## Database

Another element of a production application is the database. In this area, the presence of large language models doesn't introduce too many changes compared to classic applications. We will certainly want to record the **history of interactions with the model**, **data for RAG purposes**, and sometimes dynamic elements of prompts, such as **agent tool instructions**.

Consistency between the production and local database structure must be maintained. This ensures the `migration` process, which describes how data is organized and changes made with the application's development. However, the information stored in the database will differ depending on the environment. Here, thanks to the aforementioned `seeding` process, we can populate the database with sample data. Both the `migration` and `seeding` processes are handled by the backend developers, so if you're working on the frontend, just remember that the **production database and local database should have the same structure and possibly similar content, but should never be directly connected**.

To experience working with databases in practice, in the `database` example, I include integration with SQLite via [Drizzle ORM](https://orm.drizzle.team). Therefore, we don't need a server installation like PostgreSQL; the entire database will be recorded in a `database.db` file, which will appear in its directory after running the example.

Upon sending a query to the application at the `/api/chat` endpoint, a code fragment responsible for adding a new entry to the `messages` table will be executed, allowing us to read its content in the future.

![](https://cloud.overment.com/2024-09-17/aidevs3_entry-5c6e7fa3-3.png)

Stored data in the database can be easily browsed with graphic interfaces, such as [TablePlus](https://tableplus.com/) or any SQLite-supporting alternative. After launching the `database.db` file, we will find the history of interactions with the model inside the `messages` table.

![](https://cloud.overment.com/2024-09-17/aidevs3_db-8cf06ee2-9.png)

If databases are a new topic for you, definitely run the `database` example and examine how information is recorded in the `DatabaseService.ts` file and how the table structure is built. Knowledge of SQL is very helpful here (its basics can be quickly grasped and LLM can help here as well). You'll quickly find that databases are, in simplified terms, tables in which we organize the application's data and then read and write it with code.

From now on, most examples will use SQLite for data storage. You can also successfully work with it for private projects.
## Search Engines

In lesson S01E04 — Optimization Techniques, we went over a small introduction to vector databases, using the `faiss` vector store. However, in practice, we will rely on much more elaborate tools like Qdrant, which is available both on an excellent free plan and an Open Source version.

We will discuss vector database work techniques further in AI_devs. Meanwhile, the `qdrant` example contains a very simple implementation, saving conversation history and loading it into context. This mechanism is not particularly useful, but the key is connecting with Qdrant.

Before running the example, create a free account in Qdrant Cloud and download the database URL and API key, then add them to the `.env` file. Next, after sending the first query to the `/api/chat` endpoint, a collection will be initialized, and the first points (entries) will appear inside it.

![](https://cloud.overment.com/2024-09-17/aidevs3_collection-4c3aa233-1.png)

If everything is set up correctly, the model will be able to answer questions using the top-10 most relevant messages. From a production perspective, there are a few important details:

- Data in the vector database is stored in embedding form and associated metadata. The embedding is generated by the model (in our case `text-embedding-3-large`) and this is the first problem to consider, as the chosen model cannot be replaced with another without **re-indexing the entire database**. In the case of very large databases, this can be costly.
- Embedding, as we know, describes content meaning in number form. However, models generating it have varied efficiency depending on language (e.g., Polish/English), which also poses application development issues.
- Searching using a vector database is widely seen as insufficient (albeit very valuable). This means we also need to include other search engines and content organization strategies in the application architecture.
- Practically, we will want to **describe the same data in multiple ways** to increase search process efficiency. For example, besides the original product description, we may also want to index its individual features. Such processing complicates updating, synchronization, and later database searching.
- The vector database does not replace the classic database, so information between them must be synchronized. This introduces a risk of data stratification, which we need to consider during development.
- We know LLM expression effectiveness depends on prompt quality and included context. Since this context is dynamically built based on search results, we will also want evaluation, including monitoring the entire process's effectiveness.

Thus, the vector database is an additional application component, comparable in complexity to any other search engine. Yet here, the added challenge comes with new techniques for indexing and searching content, which we will explore further.
## The Dynamics of Changes and New Model Versions

There's a lot of discussion online about rapid LLM development and associated migration challenges to newer versions. In reality, the difficulty with switching to the latest models is often less than it seems, as they are much more efficient. Especially since, with tools for automatic prompt evaluation, we can assess new model performance for our context.

Thus, whenever a new model appears, it's worth considering its position in benchmarks (e.g., [LMSYS](https://lmsys.org/blog/2023-05-03-arena/) or [LiveBench](https://livebench.ai/)), but the ultimate decision on selection should be made after conducting our own tests.

In lesson S00E04 — Programming, I wrote about building an abstraction layer enabling the use of models from different providers. A great option is also working with tools like Vercel AI SDK or similar, facilitating work with multiple models.

Throughout AI_devs 3, our focus will be on SOTA models. However, one should also remember about Open Source models, especially those capable of running on mobile devices. Examples like the Phi or Qwen model families and Apple's Intelligence direction show that small language models can still play a significant role in generative AI.

Ultimately, regarding change dynamics, the biggest constraints can be:

- Staying up to date with models, tools, and work techniques. Lessons from S00E05 — Development can help here.
- Application architecture and design decisions tying us to chosen toolsets. We talked a lot about this in lesson S00E04 — Programming, with flexibility threads appearing almost everywhere.
- Business and legal threads can also pose serious limitations from the application development perspective. One example could be a scenario where an organization provides access to OpenAI models but lacks access to Anthropic models, which often prove significantly better.

Despite everything, maintaining high application development dynamics is mostly a technical issue, connected to the company's product culture and individual team members' development orientation. Initiatives for knowledge exchange within the organization and/or project groups are recommended.
## Conclusion

Apart from the aforementioned threads regarding application development from the technology side, the use of generative AI is not solely dependent on technical aspects. Language models' capabilities stir extreme emotions, from the most positive to the very negative. In both cases, the value gained from employing this technology is low, as we're either trying to use it where it plainly doesn't work or neglecting its capabilities entirely. It's worth reflecting on this.
Meanwhile, one thing from this lesson worth devoting time to is setting up your own private API using your chosen programming language and any tools—your own server, VPS, or serverless. The moment when a large language model becomes available to us everywhere, and we have the ability to tailor its behavior to ourselves, marks the beginning of the AI_devs 3 path.