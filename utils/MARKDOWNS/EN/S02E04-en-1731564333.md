At a certain stage, multimodality is likely to enable us to generate text, audio, images, and possibly even video. An example might be [Advanced Voice Mode](https://www.youtube.com/watch?v=MirzFk_DSiI). Thus, we are talking about the possibility of **speech-to-speech** interaction and other configurations.

On the other hand, we may want to interact with models specialized in specific tasks or models that do not have the capability to process multiple formats.

In both cases, we strive to transform data appearing in various formats. Sometimes these will be extensive product databases that require description. Other times it will be a loose voice note that needs structuring or a video from which we want to extract some information.

## Generating Text and Audio

In lesson S01E03 — Limits, I mentioned that large language models are much better at transforming content than generating it. We are talking here about the situation where **from a few words of description, we expect a developed document** that maintains style consistency and a high level of substantive content. However, this does not mean LLMs (large language models) are incapable of creating high-quality results, though it requires a bit more involvement in crafting the logic guiding the model.

A good example of such model guidance is **content summarization**, which is often presented as one of the model's basic skills. On the animation below, we see two summaries: the first one generated by a simple prompt asking to summarize one of the AI_devs lessons. Whereas on the right, we have the result of the `summary` example, which builds the summary multi-step, also considering links and images interspersing the content at appropriate places (which is not always obvious for the model).

We are talking here about compression at the level of ~55%. Moreover, the summary is divided into sections (introduction, discussed topics, key conclusions), so one can only familiarize themselves with the parts that interest them.

So the question is: **how did I create such a process?**

First and foremost, I started with the question of **how I myself would write such a summary**, but instead of answering it myself, I discussed it with Claude 3.5. However, it did not happen within one thread but multiple:

- Initially, we used the "First-principles Thinking" mental model to identify the steps needed to create a good summary. The result contained useful hints (e.g., scoring or topic modeling) that I wouldn't have thought of myself. However, I didn't like the overall process, so I **asked to generate a note from our conversation, which I used to start a new thread**.

- The new conversation started with the generated note from the previous discussion. It allowed for context transmission, serving as a basis for discussing each step individually. **The suggestions from the model varied and most of them didn't make much sense**, but again there were hints I wouldn't have thought of myself. The conversation also helped me structure my own thoughts.

- As a result, I arrived at the following description of the main steps (extract, draft, refine, critique, summarize, refine) and in the case of the first step, also subpoints. The developed plan looked fine, so it just needed translating into code.

In the `app.ts` file of the `summarize` example, there is a function that parallelly performs independent extraction of information from each category. This way, we build **context based on which the first version of the summary is generated**. Next, it is **criticized** considering both the original article content and all extracted information.

The draft, collected context, and critique are then passed to the prompt writing the main version of the summary, which ultimately undergoes a revision in terms of style. 

Collectively:

- Currently, language models are quite limited in performing complex tasks within a single query. On the other hand, if we focus their attention on one activity only, the effectiveness is very high and can reach 100% depending on the task.
- The text processed by the model can be analyzed multiple times from different angles. This allows for automatically gathering a high-quality context based on which the target task can be performed.
- However, it is worth considering additional steps where the generated content is criticized or even verified against our requirements. Such generated feedback will eliminate a large portion of errors present in the first iteration.
- The final summary should already have the target structure and complete information. However, due to LLMs' tendency to use relatively imprecise expression style by default, it's worth taking one more step that focuses solely on content styling.
- **IMPORTANT:** The steps listed above do not have to be carried out by a single model. On the contrary, it is advisable to combine various models — from smaller and cheaper ones capable of performing simple operations, to the most advanced ones capable of building complex narratives.

The effects can be seen after running the prompt or by viewing the content of the markdown files present in the `summary` directory. In the meantime, I would like to point out another element used in the code, namely **giving the model time to think** by applying `xml-like` tags, within which a response is added after prior consideration. This is a practical combination of information from [Let me speak Freely?](https://arxiv.org/abs/2408.02442v1), techniques suggested by Anthropic, and Chain of Thought.

Ultimately, the prompts used in generating summaries can be tailored to us. We can also think about preparing instructions that are universal enough to work in various sequences.

It should also be noted that the `summarize` example is capable of generating summaries of limited length, stemming from both `input` and `output` token limits. Additionally, there may also be issues with rewriting links and the typos that appear in them. However, we will return to this topic in the later part of AI_devs.

## Describing Images and Video

In the summary generated by the `summary` example, images are included, specifically the links that lead to them. According to lesson S01E02 — Context, hosting files on one's server is crucial for the model to use them in its expressions, using markdown format.

The links alone are not sufficient, as in this form the model cannot preview what is on the image. We therefore need to retrieve the file content and provide it to the model in base64 format.

Just like for the summary, we need additional context to generate a valuable description. The information we need includes **image preview** and **context in the form of surrounding content**, which the model can combine into a cohesive whole.

Speaking specifically:

1. We load the content of the document.
2. Using regular expressions, we fetch all images and load them in base64 form.
3. We gather context through two queries to the model: **preview** and **context**.
4. Finally, we merge these two pieces of information.

Below is an example where the combination of visual interpretation of the image, along with its surrounding context, is clearly visible. This is the result of the [captions](https://github.com/i-am-alice/3rd-devs/tree/main/captions) example.

Such generated descriptions can be used in the `summary` example to enrich the context based on which we generate the description of the entire article.

Moreover, we are not limited here only to images, because in the case of the Gemini 1.5 Pro model, we can also analyze video material. Below we have an example from [Google AI Studio](https://aistudio.google.com/) where, based on a fragment of a recording about ComfyUI, the model correctly listed the mentioned tools.

The schema for describing video materials looks similar to that of images, and Google AI Studio provides an API that will allow us to automate this process. We just need to pay attention to the number of tokens, which for a one-minute video amounted to over 18,000. The interaction with the Google AI Studio API is found in the `video` example, which contains two test files `test.mp3` with a short recording and `test.mp4` with a few-second video. To switch between them, simply change the name and `mimeType` in the `processMedia` function.

**IMPORTANT:** Google AI Studio does not offer easy cost control, only limits on the number of queries for individual services. In the `video` example, I included a file size limit of 2MB, though the API itself allows transferring files weighing up to 2GB.

The last important piece of information concerning working with Gemini models is the ability to use cache memory for context, which, in the case of multiple audio/video file processing, is a necessity.

## (Partially) Autonomous Collaboration of Different Models

In the examples so far, we used various models, but their collaboration was individual in nature. For example, one model generated text, and another converted it to audio, or vice versa. While there's nothing wrong with such a situation, we can go much beyond this.

Namely, we can programmatically combine model skills so they collaborate with each other with the aim of achieving the best possible result. We saw a glimpse of this in the `summary` example, where the `o1-mini` model verified the work of `gpt-4o`, but there we moved only in the text area, slightly venturing into image analysis.

**NOTE:** The narration example relies on ElevenLabs' currently experimental capabilities of generating sound effects. Using them is currently quite expensive. In this example's directory, there are sample files showcasing the effect of its operation.

However, in the `narration` example, models `o1-mini` (OpenAI), `eleven_multilingual_v2` (ElevenLabs), and `gemini-1.5-flash` (Google DeepMind) are tasked with generating a narration on a given topic, enriched with sound effects.

Such a scenario will probably soon be very simple to realize with the help of multimodal models, which may perhaps be capable of generating sound. However, at the time of writing these words, it is unfeasible with the help of one model.

We are talking about the following scenario:

1. The user sends a topic or narration to the model, and the model is to generate a response in the form of narration enriched with sound effects.
2. In the first step, with the help of the model, we enrich the text with `[sound descriptions]`, which can reflect the current scene, e.g. "He walked at night in the snow `[calm walk on a snowy sidewalk]`, when suddenly (...)".
3. Then, programmatically, we segment the text, grouping the fragments of the narrator's speech and sound effects.
4. Having the data arranged like this, we can use Text to Speech and Text to Audio to generate a soundtrack that can be assembled into a whole.

It doesn't sound too complicated, except for the fact that the quality of Text to Audio models is currently low, and it's difficult to generate a recording that meets our expectations on the first try. This is where `gemini-1.5-flash` comes into play, with the ability to recognize not only speech (like Speech to Text models) but also sound. Below, we see how the overall mood of the recording is determined in the `_thinking` property, and the fact that it doesn't fully match the expected style. Such an observation leads to the decision to regenerate this fragment.

![](https://cloud.overment.com/2024-10-02/aidevs3_listen-ab3d4eb1-0.png)

Examples of narrations generated in this way are located in the `narration` directory, and one of them can be listened to below. If you want to test this mechanism, it will be necessary to complete the keys in Google AI Studio and ElevenLabs, and then replace the user's message from the `generateNarration` function in the `narration/app.ts` file.

<div style="padding:100% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1015122040?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="02_04_narration"></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>

The production use of this mechanism, which would operate on a somewhat larger scale, **would require a user interface allowing for manual corrections**. Full automation would be possible, provided access to a more precise model responsible for sound effects.

Ultimately, this example is intended to **highlight the unique value that comes from connecting multiple models**. Similarly, we can look at other formats, developing tools capable of generating reports, podcasts (like [NotebookLM](https://notebooklm.google/)), or videos.

Some examples:

- A multimodal RAG allowing work with various formats, including video, and even real-time recordings
- A multi-format learning assistant capable of both analyzing and generating various formats. With tools like [markmap](https://markmap.js.org/) or [mermaid](https://mermaid.js.org/), it could visualize concepts using graphs and mind maps
- Tasks such as speech or image recognition can be performed in parallel by different models, which show varying effectiveness depending on the situation. This allows for increased effectiveness where a single model is insufficient
- The reasoning process itself, carried out by several models simultaneously, also results in significant improvements in effectiveness. An example is [Mixture of Agents](https://arxiv.org/abs/2406.04692) where Open Source models achieve better results than, for example, GPT-4.

![Mixture of Agents Structure](https://cloud.overment.com/2024-10-02/aidevs3_mixture_of_agents-5f12200a-f.png)

## Summary

Multimodality significantly enhances the usability of generative AI models, which, going beyond mere text, can autonomously perform tasks that previously required human involvement. On the other hand, the current development of models does not yet allow for freely addressing any problem. Additionally, models occasionally generate content containing errors, meaning human involvement at some stage of the process is necessary.

Probably the biggest challenge related to the practical application of multimodality remains our **creativity** and programming skills. This is related to the fact that we are talking about entirely new possibilities, where it is quite difficult to "connect the dots". This includes the aspect of the technology itself and the application of APIs and tools we don't need to work with on a daily basis (e.g., WebSockets).

The most important topic of this lesson is the advanced document processing presented in the `summary` example. So if you want to take only one thing away today, run it and test the operation on a blog entry you read and check the quality of the summary. If the result does not meet your expectations, think about what you can change to achieve it.

Good luck!