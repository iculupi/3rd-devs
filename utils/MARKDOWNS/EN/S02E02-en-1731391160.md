Multimodal models, such as GPT-4o and Sonnet, are capable of processing not only text but also images. This allows us to perform advanced image analysis, utilizing all the model's skills. The entire process can be controlled programmatically using access to the file system, system clipboard, network access, or mobile device cameras.

Vision language models also have limitations, such as hallucinations or context window limits, as well as some additional constraints related to image processing itself. Conversely, we can utilize similar prompt engineering techniques like Self-Consistency or Chain of Thought.

In this lesson, we will cover the basics of working with vision language models, allowing for their practical application in building AI agents and self-automations.
## Image Processing API and Working with Files

Similar to text processing, a query to a VLM is created using the ChatML format, but the `content` property is not a string but an array that can contain `image_url` and `text` objects. Thus, we can send **multiple images in a single conversation**. However, we must remember that the LLM API is **stateless**, meaning images will be interpreted **each time**, which can result in significant costs.

We see that the message list structure looks different and allows us to include images in `base64` format. In OpenAI's case, we can also specify detail (the `detail` parameter), or even provide a URL pointing directly to the image. However, it's usually best to stick with `base64`.

While the interaction schema with VLM usually remains the same, there are slight differences in the query structure for Anthropic. This is one reason we might want to use the APIs or ultimately frameworks mentioned in lesson S00E03, especially if we want to work with models from different providers.

In the `vision` example, there's code where VLM (in this case, GPT-4o) responds to a question related to a screenshot of the AI_devs 3 lesson, tasked with listing the number of comments and likes visible under each lesson.

After running this example, we see a response in table form containing the values requested by the user. Moreover, the console also displays the token count, calculated for text using the Tiktoken tool and for image by a function based on [OpenAI documentation](https://platform.openai.com/docs/guides/vision).

Additionally, several background actions occur, such as **image compression** and its scaling while maintaining proportions. Scaling is important because Vision Language Models can only process images of specific sizes. We must consider this in the context of the model's ability to recognize details.

## Current Capabilities of Vision Language Models

While image recognition abilities initially seem impressive, their limitations soon become apparent. Some of these are described in the publication "[Vision language models are blind](https://vlmsareblind.github.io)," essential for understanding model limitations. A primary example is difficulty recognizing intersecting shapes or navigating a chessboard.

It’s also valuable to review [GPT-4 Research](https://openai.com/index/gpt-4-research/) and [GPT-4V System Card](https://cdn.openai.com/papers/GPTV_System_Card.pdf), which outline the general capabilities of models. However, working with VLM requires testing its capabilities on test data.

Another notable publication is "[Eyes Wide Shut?](https://arxiv.org/pdf/2401.06209)," pointing out limitations in recognizing direction, rotation, or events governed by physics laws.

From my observations:

- VLMs can recognize colors but can't provide an actual palette, e.g., in HEX format. Therefore, color issues should be addressed programmatically.
- VLMs can't determine image size or distances between elements or their exact location. Models like [Segment Anything](https://segment-anything.com/) from Meta can be helpful here.
- VLMs have a knowledge cutoff, meaning they can’t recognize things beyond their baseline knowledge, except when an object's description provided in context allows identification.
- VLMs are also subject to prompt injection, which can be included directly in the image content.
- VLMs excel at text recognition (OCR), although errors still occur.

However, we must remember that VLM development is rapid, and subsequent versions show increasing capabilities and precision in image and even video recognition. Presently, seeing VLMs as models capable of **general image understanding**, excluding precise details, is appropriate. Yet, considering the new capabilities of the Claude 3.5 Sonnet model (October version), this statement may soon become obsolete.

## Prompts for Vision Models

I mentioned that creating prompts for VLM allows leveraging techniques known from text processing. Examples like Few-Shot, Chain of Thought, or Self-Consistency combined with images are effective. Additionally, when the model allows, we can also use Function Calling and JSON Mode.

In Anthropic's [Cookbook](https://github.com/anthropics/anthropic-cookbook/blob/main/multimodal/best_practices_for_vision.ipynb) repository, several example instructions for VLMs can be found. One example involves an image with a question about the area of a circle. The model can correctly read and answer it.

This demonstrates that Vision Language Models can understand image content well, including text and additional objects. This includes elements added programmatically (e.g., arrows, frames, grids).

Regarding the instructions for VLMs, beyond classical language model techniques, we can use phrases like "you have perfect vision" or "take a closer look." We see this in a prompt example from Anthropic's repository. Naturally, they won’t cause models to magically see every detail, but they increase the likelihood of a correct task completion.

A VLM prompt involves not only text but also image(s). How we mark and/or crop content will matter. Moreover, problems with models like GPT-4 recognizing images can be addressed by supporting models specialized in specific tasks, such as image segmentation.

In the `segment` example (requiring a Replicate API key), I used a screenshot of my screen showing AI_devs 3 main page and the Alice application. The GPT-4o model can answer most questions related to visible screen content quite precisely.

However, if the question is ambiguous or concerns a minor detail, response quality declines. For instance, **if you ask the active model, you get "Alice," which is incorrect as it’s the avatar's name, not the model.**

To change the query context, I used the [adirik/grounding-dino](https://replicate.com/adirik/grounding-dino) model available on Replicate. Using it, I highlighted the "Alice" window, resulting in the following outcome. The screenshot is marked with frames describing elements matching the prompt, and the API returned each frame's coordinates.

Just recalculate the values for image cropping and save the result in a new file.

Re-asking the question with the cropped application window yielded the correct model name, "Claude 3.5 Sonnet."

The conclusion is that even if Vision Language Models struggle with processing a selected image, we can modify the query context by changing the instruction, modifying the image, or even adding additional ones to help the model better understand our issue. The [Roboflow](https://roboflow.com/annotate) platform, offering models specialized in segmenting and describing images and videos, can also be helpful here.

## Object Recognition

Publicly available VLMs are limited in recognizing known individuals. However, I mentioned the possibility of providing a description in the context of a prompt, which the model can associate with the uploaded image. Similarly, we can add other images as reference points.

In the `recognize` example, there's an `avatars` directory containing 9 avatars of my AI agents, but two depict "Alice," differing slightly. Naturally, GPT-4o doesn't know about my projects, so I need to provide context knowledge.

I generated an Alice description using the model, requesting a first-person perspective description of the `Alice_.png` image. It’s quite exhaustive. Each detail will aid further identification of this character.

In `recognize/app.ts`, there’s the `processAvatars` function reading files from the directory, processing each with OpenAI and the GPT-4o model. The system prompt request contains "It's me" or "It's not me," depending on whether the image matches the description.

Running the script displays a summary table with a correct classification of two images, indicating the description was correctly matched to the proper images, even though one of them was depicted differently from the original.

Of course, we can’t expect 100% accuracy, but I believe this example showed potential image classification possibilities applicable in business processes — customer service, sales, or document analysis.
## Open Source Models

A few days before writing these words, this section of the lesson would have looked completely different. However, Mistral has released an excellent VLM that can realistically be compared to the best available Vision models in terms of precision. The model itself is available both [via API](https://console.mistral.ai/api-keys/) and [on HuggingFace](https://huggingface.co/mistralai/Pixtral-12B-2409).

Working with locally operating VLMs is not particularly different from working with commercial models. Naturally, one must remember their limitations and correspondingly lower effectiveness.

In the `recognize_pixtral` example, I used the exact same logic as in the `recognize` example. However, it was necessary to slightly adjust the system prompt by introducing a length control instruction. And while the task was completed correctly (as shown below), Pixtral does not always accurately recognize both photos.

![](https://cloud.overment.com/2024-09-25/aidevs3_pixtral-555b87b4-9.png)

Nonetheless, this does not change the fact that locally operated VLMs can already be effectively used in building tools that require image recognition. Besides Pixtral, Vision models from the `Phi` or `Qwen` families can also be considered.

An additional advantage of Open Source models is the possibility to adapt them to your needs through Fine-Tuning, although this topic is beyond the scope of AI_devs 3.

## Summary

By now, it should be obvious that Vision Language Models offer great potential related to image understanding, although they still have many limitations that can sometimes be difficult to circumvent.

It is noteworthy that the effectiveness of VLM operation will vary not only based on our instruction but also the transmitted images, their compression, or depiction through graphic markings.

Furthermore, even if VLM does not perform well in a given area today, it does not mean that future versions of models will also face this issue. Therefore, it is worth observing the development of models from OpenAI, Anthropic, or Google DeepMind and periodically reviewing their progress.

If there's one thing to take away from this lesson, it would be to create a simple script using the code from `recognize`, with the task of OCR image recognition. In situations where the text is unreadable or absent, the model should return 'no text'.

Good luck!