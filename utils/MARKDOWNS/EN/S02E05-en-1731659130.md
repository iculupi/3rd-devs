![](https://cloud.overment.com/2024-11-14/s02e05-9a2a5422-c.png)

In the previous lesson S02E04 — Connecting Multiple Formats, we saw the value derived from the simultaneous application of multiple models, especially in the context of multimodality. However, now we will look at several examples of translating this value into practical, everyday applications.

Our goal is not merely to build a few more tools but to look from a broader perspective at the scheme of their creation. This will allow us to transfer them to other areas and solve various problems. We will also find that some often very useful AI products are straightforward to replicate.

## Structuring Text for Audio Needs

I've mentioned several times that LLMs (Large Language Models) handle content transformation much better than its generation. This may suggest that a task like turning an article into audio form using Text to Speech should be simple. However, it turns out that's not entirely the case because we need to consider several important elements.

Firstly, depending on the models used, we will be subject to limits on both input and output data, which we will examine more in the third week of AI_devs.

Additional problems will include images and external links that require generated descriptions, as we saw in the `captions` example and working with VLM (Vision-Language Models).

The ultimate challenge is also creating a suitable prompt that matches the model's speech style so that we don't end up with a "fascinating and exciting experience" (phrases often used by the model in content transformation) or references to the "below image," which is hard to see in a recording.

In the `read` example, there is code capable of converting our lesson content into a listenable audio form while maintaining the original speaking style. In the `demo` and `demo_2` subdirectories are recordings generated with the help of ElevenLabs. Its logic is divided into three steps in this case: mapping style, converting the original article, and generating an audio file (for simplicity, I've omitted the `caption` example logic for describing images).

A key role in such transformations is played by a prompt, which should not indicate generating new content based on the current one (e.g., "generate a podcast based on the document below") but rather its **transformation** according to defined rules.

![](https://cloud.overment.com/2024-10-04/aidevs3_toaudioprompt-f83f6732-4.png)

In all this, we want to include one more very important detail related to translating into Polish — word modifications. An example of a typical model error is visible below and is usually associated with an attempt to translate expressions from English without ensuring their appropriate form.

Here too, we can largely fix this by making changes in the original instruction that highlight the necessity of taking care of proper forms or through an additional prompt focused solely on translation correction.

## Mind Map Conversation

In lesson S02E04 — Connecting Multiple Formats, I mentioned an example of "conversation with a mind map," whose basic implementation is in the `mindmap` directory. This allows us to turn a voice note into a mind map visible below. Despite the recording being very chaotic and at times difficult to understand, the model correctly recognized all mentioned concepts and translated them into a visualization.

The example's logic is very simple and doesn't contain any new elements aside from using `markmap-cli` to generate a mind map in Markdown and HTML form. As usual, nothing prevents the processing from being more extensive and even considering Internet connection to enrich our visualization.

Alternatively, we could use the `audio` example interface (a new version of which is in the `audio-map` and `audio-map-frontend` directories) to "talk with a mind map" exactly as seen in the video below.

The above video is slightly sped up, but the reaction time is still quite low. We also have room for optimization using Whisper Turbo and ElevenLabs Turbo. Any prompts responsible for building the mind map could undergo further refinement, which in the case of advanced implementation could include some form of "diff" mechanism. In that case, there would be no need to update the entire map, but only its fragments.

The "voice to mind map" logic can be translated into a real-time interface, particularly using platforms like Groq, known for fast inference. A good example is [a post from the profile](https://x.com/Mlearning_ai/status/1816065857228767474) `@Mlearning_ai`, which presents a recording of a conversation with a table structure showcasing a trip plan.

![](https://cloud.overment.com/2024-10-04/aidevs3_voice-11c7027d-a.png)

## Structuring and Formatting Text from Audio

We can already see quite clearly how multimodality fares in practice, both positively and negatively. Ultimately, despite all these limitations, we can still confidently talk about building solutions that bring value. However, currently, in search of quality, we often have to sacrifice reaction time. Yet, time doesn't always have to be crucial and can directly result from our design assumptions.

A first-hand example could be **voice notes** that some people already use, although this requires manual transcription or possibly working with transcription, which isn't always perfect.

The creator of [Audiopen](https://audiopen.ai/) noticed this issue, focusing on formatting the dictated note alone, offering the possibility to adjust length or style. While we can't deny the effectiveness of implementing this idea, it's clear at this stage that the main underlying logic is relatively simple to replicate.

![](https://cloud.overment.com/2024-10-04/aidevs3_audiopen-94e20f7f-1.png)

Moreover, with our implementation, we can take it a step further and fully tailor the way audio recording transformation occurs. As we'll find out later, this will also form a potential element of the AI Agent collaborating with us.

The greatest value from deploying the above scheme comes from the ability to adjust it to personal preferences and give it our context. After all, each of us may approach task planning, shopping, or taking meeting notes differently. Additionally, with such a scheme, one may go a bit further, for example, towards keeping a diary or a workout plan.

The following note fragment was built based on a short message describing a meeting with Grzegorz about the Alice project.

The thing is, this name wasn't mentioned at all in the note's content, but it could be inferred from the context. You can see its content in the `notes` example among one of the sample messages.

The ability to infer from context is possible because this context was read after determining the note type (task/meeting note/shopping list). We implemented similar scenarios in the `linear` example from lesson S01E02 — Context, where we focused on ensuring category descriptions were clear or allowing the model to navigate ambiguity resolution.

Additionally, the fact that we applied a message type recognition step allows us to load matching context and formatting preferences. It also results in another value of a **single entry point**. This means that every voice or textual note directed to this tool will allow its proper formatting and potential placement in the appropriate category in a notebook.

## Combining Information Sources from Different Formats

Vision models are increasingly adept at recognizing details and inferring based on images. Thus, we can use them even in the context of noting as a data source or providing context. The context itself can also be given by data from the device we're using to create the note (e.g., phone location). Hence, the updated interaction scheme appears as follows:

It includes combining audio recording (or a text message), a photo, and metadata from the device. All of this can be passed to the model and then associated with the correct category or the model's long-term memory.

It's worth recalling VLM's ability to match descriptions to an image, as seen below, in a message that correctly identifies my office. In the same way, we can associate places or items (e.g., receipts).

We're not going to deal with the actual implementation of combining the image and device metadata because it will depend on the operating system and the device itself. For example, in the case of an iPhone, we have the Shortcuts app, which allows creating a macro to take a photo, record a voice message, and send this data to a specified address. As you can see below, it's incredibly simple.

## Summary

The examples discussed in this lesson aimed primarily at showing a mindset that strives for creative, yet useful connections between language model capabilities and functionalities of the devices around us.

Even if none fit your everyday life or professional context, consider what tool you can develop for your needs. This is important as it will allow for practical experience of the current models' capabilities. Additionally, such simple solutions often also allow for quite simple switching to new model versions.

The greatest value of the above capabilities, and a generalization of what we're discussing, is **content transformation** between different formats. We often encounter practical situations where some documents (including emails) need to be unified (e.g., to CSV format) or comprehensively described. The role of large language models turns out significant then, even despite potential errors they might make. The time required to verify content is always generally less than creating it from scratch.
If there is only one thing you take away from this lesson, simply look around and point to one example of how multimodal models can help you in daily life. Share your idea in the comments.