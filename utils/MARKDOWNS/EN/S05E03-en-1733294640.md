![](https://cloud.overment.com/2024-11-04/s05e03-ee52ce90-9.png)

The language model can use tools, expanding its knowledge and gaining new skills. Furthermore, its ability to process natural language allows it to **indicate a tool** and **describe how to use it**. This is a mechanism we have had the opportunity to encounter on various occasions.

Connecting with tools has become so important that OpenAI, Anthropic, and most other platforms offer dedicated functionalities to facilitate this process. We are talking here about so-called function calling / tool use.

We thus have the ability to provide a **list of tools**, their descriptions, and **data schemas**. Based on this, the model can decide what action to take at any given moment.

The `tool_use` example includes **two tools**: **web_search** and **send_mail**. I skip their implementation here and return default values, as nothing more is needed to understand Function Calling.

Each of these functions must be described with the help of [JSON Schema](https://json-schema.org/) (but beware, OpenAI [does not support this syntax 100%](https://platform.openai.com/docs/guides/structured-outputs#supported-schemas)). The structure of the object itself can be generated with the help of LLM, but we must focus on three things:

- The names of functions and properties
- Descriptions of functions and properties
- Maintaining **uniqueness** between names and descriptions to reduce the risk of confusing functions

An example of a JSON Schema for the **web_search** function looks like this:

We see that it is something we have built so far, but described in a structured form.

Such prepared descriptions of tools can be added to the conversation. Below, in **line 81**, there is the **tools property**, which is **a list of tool schemas** that the model has access to.

The structure of these tools will be attached **to the system prompt**, but OpenAI does not share details on how exactly this is done. However, an important piece of information is that the schemas **are part of the prompt reaching the model**.

The result of this logic at the current stage is **an assistant's response** with the `content` property set to `null`. However, we receive an array of `tool_calls` containing objects describing the function to be executed along with a list of arguments.

Naturally, **this does not mean that the function was executed**, as the model does not have that capability, and the execution itself occurs on the programmer's side (as we have been doing so far). However, we have everything we need to execute the function.

Similar to examples from previous lessons, we use the data returned by the model to execute the function. But beyond that, we do something else. Namely, we add two items to the list of messages. One in line 88 and another in line 102 in the screenshot below. **This way, the conversation includes information that a function was called and what the result was**.

And basically, this is the most important information we needed from Function Calling. That is, **information about using tools enters the conversation**. 

## Function Calling vs Own Code

Function Calling presented in the `tool_use` example effectively structures the code and allows for convenient transmission of information about the executed action to the model. However, throughout the AI_devs 3 training, I have shown a different approach, involving self-creating logic responsible for deciding on function selection and generating arguments, as well as later passing the results to the model.

This did not happen without reason. The knowledge we already have allows for easy use of Function Calling. However, my experience tells me that it will not always be the best option. Specifically:

- Function Calling **combines** the stage of selecting a function with **generating parameters for it**. In the case of agents, we will very often need **time** between one and the other action. That is, we first decide on the selection of the tool and only then on generating the data to use it. An example can be the selection of the linear tool, which, to perform the action, needs a context in the form of a list of tasks from the last few days.

- The process of building arguments for function calling itself can consist of many stages. Again, Function Calling makes this difficult for us. 

- Function Calling is by default executed within a single thread. Otherwise, it is necessary to transfer the function call values + the result returned by the tool. Otherwise, we will receive an error.

- Function Calling (but only in the case of OpenAI) limits the description of the tool to 1024 characters. It is difficult to say how long this limitation will be maintained, as it once also applied to parameter descriptions.

- Function Calling requires predefined function schemas. The thing is, we will not always know them, as they may change depending on the context.

- The structure of Function Calling looks slightly different depending on the provider. If we use two models within one application, it complicates logic.

Thus, Function Calling has its undeniable advantages, but also annoying disadvantages, which become apparent, especially when building an advanced AI agent. Then it turns out that details such as instant function selection and argument generation prevent us from implementing the full logic.

For this reason, I myself avoid using Function Calling in favor of my own implementations. They are not difficult to implement, and at the same time, their effectiveness is high. **However, this does not mean that it is a universal practice** and that it is not worth using Function Calling. You just have to keep in mind the limitations of this functionality.

If you consider my view on this topic to be incorrect or your projects allow you to use Function Calling, then as a tip, I will emphasize the fact that when generating JSON Schema, the model itself is very helpful, especially when we provide it with the context of OpenAI documentation on the subject. Besides, all the knowledge we have acquired so far translates directly to Function Calling.

## Tools in Agent Logic

We have talked enough about building tools in S04E03 and S04E05 lessons, but now we will focus on them in the context of agent logic. 

Let's start with the fact that the current perception of agents often includes their unlimited capabilities and ability to perform extremely complex tasks autonomously. However, reality does not look like this, or at least not at the moment, due to both the current capabilities of models and the typically technical limitations of the API and infrastructure.

For this reason, **specializing the agent** in a specific area is a good idea. Moreover, such an agent usually **should not have contact with the end-user** or this contact should impose certain restrictions. Of course, there are exceptions, but in their case, we must keep in mind all the challenges associated with implementing such a system, which I have mentioned many times.

Specializing the agent allows for the precise selection of tools and describing how to use them, including the relationships between them.

Below is an example of a set of tools for an agent capable of searching the Internet, creating files, uploading them to the server, and sending them by email. It is a fairly simple set that the model will be able to use without much trouble.

So if the task is set before the agent:

- to search the indicated sites for the mentioned books,
- create a file with a list of these tools
- upload it to the server
- send it as an attachment in an email

The above plan can be generated by the model and correctly implemented, **but** we can also consider **supporting this logic** on the programmer's side. The only question is **how?**

If writing a file **always** means uploading it to the server, then there is no point in adding a separate tool for this purpose. Not only will this increase the time needed to complete the task, but it will also create space for possible error.

Of course, this approach will not be possible if the **write** and **upload** tools will not always connect. An example can be a situation where the content of the written document may sometimes be transferred to another tool.

We thus see that we need to determine **what kind of tasks our agent will perform**. Based on this, we will assign tools to it that it will be able to use in various configurations. We just need to remember the conclusions from previous lessons related to **tools not being able to overlap** and **having a consistent interface for input data and results**, thanks to which we will be able to embed them in the agent's logic.

Let's look initially at the scheme depicting Function Calling. Blocks with a black background are part of the message list `messages` exchanged between the user and the model. The block without a background is a programmatically executed action that is not part of the conversation (but its result is).

In the case of an agent, the scheme looks slightly different because it includes actions for which the result will be needed **but only within the current task** and those that will remain relevant **for the entire conversation**.

In other words, we have three types of blocks here:

- Those whose content remains throughout the current conversation thread (black background)
- Those whose content remains in the current operation (translucent background)
- Those whose content is immediately removed or stored in another form (no background)

Therefore, when designing an agent, we need to plan this, **on how long we keep certain information** and **how the agent can use it**.
## Decision to Take Actions and Describe the Action

In the previous lesson, in the `assistant` example, we had logic responsible for laying out **tasks** and related **actions**. Both the **planning** and **execution** stages are run **in a loop**, allowing it to be updated during execution and react to current events.

Thus, after generating the plan, it is passed to the **execution** stage, where the action to be triggered next is selected. Then, the model generates the request object needed for the function call. Thus, unlike Function Calling, the model at any given moment focuses either **on the function selection** or **on generating its parameters**.

The `assistant` example thus contains a separate function that specializes solely in **building a request object** for the currently active task action. In other words, to prepare for the next step.

The request object should have **the simplest structure possible**, and the prompt generating it should have **the minimum necessary information** to create it. In situations where this is not possible and the request object needs to be complex, it is a good idea to use [Structured Output](https://platform.openai.com/docs/guides/structured-outputs), which **guarantees** the object's compliance with the structure (though it naturally does not guarantee that its values will match our expectations).
When we look into the prompt of this example, we will see that we have information **only about one tool** and **the current action**. Therefore, it lacks knowledge of the results of previous steps, but it shouldn't matter because all necessary data should be included in the user's query.

For example, if in the first step we retrieved a list of three concepts such as Transformer, Encoder, Decoder, then in the second step, a search query should be: "**Find information about: Transformer, Encoder, and Decoder**".

However, it may happen that adding information about previous steps will be required. An example could be the command: "**Go to this site `https://...` and download data from it and send it to me via email**." In this case, to send the email, **we must have data from the previous action**.

The conclusion is simple — the structure of the agent will differ depending on its specialization. Although we can already discuss general-purpose agents, my experience suggests that it is still too early for that due to model and tool limitations. However, this may change in the coming months due to the industry's strong focus on agents.

I don't know if it's clearly visible, but in the logic of the agent, **we programmatically select data** and **use it in several smaller queries**. Of course, this will not always be necessary, and sometimes even undesirable. If we can perform some operations with one query and achieve high effectiveness, we should do it. Conversely, when the agent's effectiveness decreases, finding a way to divide its activities into smaller steps is usually a good direction.

Thus, to summarize:

- The execution stage should be separated from the planning stage. Execution itself must focus the model's attention solely on running the tools, not deciding which to choose. The exception is simple logic that the model can handle within a single query.
- The data structures needed to execute tools should be as simple as possible to reduce the risk of error.
- The amount of data the model works with when using skills/tools should be minimized while still including all necessary information.

## Responding to Actions and Errors

Since in the `assistant` example we only have sample tool implementations, I will use code fragments from one of my projects here.

Below is **a function** that is invoked by **every tool**, but based on its arguments, the target service and function are selected. In other words, we have a method here **deciding** how to handle the specified tool and the generated request object.

Moreover, we immediately see that in case of an error, I return here a **document** containing information about **failure**. The document's structure is the same as the ones we use for working with files or external knowledge sources. The question is — why?

Well, I always say that in the logic of the agent, we should aim for a consistent data format. This way, it will be possible to connect different modules without the need to create dedicated connections. Data will just flow freely between them. That's why in this case **I made the decision for this particular data structure**.

So, this results from **my decision** and **the needs of a given project**, not from a standard. One assumption that must be maintained here is consistency. That's all.

So, looking at the code written above, it doesn't matter whether my agent will use 1 or 50 tools (with a larger number, the decision-making process may be more complex). I just need to update the list of available services, and the same code will serve them all. Furthermore, regardless of whether an error occurs or they are launched correctly, the same data format will ensure that the agent is properly informed about such an event.

## Plan Update with Progress

If the main logic of the agent (the one executing in a loop) is correctly connected with the state, and the dynamic elements of the prompt correctly load all information, the plan update will happen automatically. Just new data will enter the context, and the model will respond accordingly. However, there are a few things here that deserve attention.

Firstly — **organization of information**.

Until now, **the whole context** was transferred to the system prompt. This means that the information contained in it is **above the message list**, suggesting that these events happened **earlier**.

We see this in the diagram below, where on the right side is a preview of the "Plan Next Step" stage. It is clearly visible that the list of steps taken is **before** the user's query.

Such organization of information **may** (but doesn't have to) disrupt their understanding of the instructions and lead to **the repetition of the same actions**.

Example: **The user asks to turn off the lights**. The agent performs this action, and the information about it goes **to the system prompt**.

Here, from the model's perspective, the user **still asks to turn off the lights**, because the request is "later" than the information about the action taken.

And at this point, what Function Calling suggests, namely **adding as assistant/tool messages** to the conversation, proves useful. This way, the list of actions taken appears **after the user's query** and makes logical sense.

Similar situations can be encountered in various situations, especially as the complexity of the agent's logic increases. I mention this to highlight the importance of maintaining order, consistency of data format, and as much precision as possible in the application code.

Regardless of whether logic error occurs on the programming side or the model side, we can use LLM for its resolution. I have repeatedly found that even "debugging" the prompt is much easier when discussing the problem with the model.

Below we have a snippet of a statement in which GPT-4o helped me understand why in some cases, the agent decides to choose an Internet search engine, even though the user doesn't mention current information. The model, however, pointed out that the term "modern neuroscience" can be interpreted as requiring network access, and to some extent, it's hard to disagree.

Similarly, the model also helped me solve a problem regarding looping logic when I first encountered the problem of the order of executed tasks mentioned earlier.

Continuing further, the model helps me **in shaping the prompts**, in **describing tools**, **planning data structures**, and arranging **the entire agent logic**. This support takes place over time and through a standard, iterative approach to solving it. In other words, **we focus here on designing such systems, leveraging the capabilities of current models.**

## User Interaction and Response

In the `assistant` example, the agent's contact with the user can occur in two situations: **when the agent decides so**, or when the **allowable iterations limit of the loop is exceeded.** In both cases, it's about invoking the "final_answer" action.

However, there may be more situations in which we programmatically enforce contact with the user. One of them might be **confirmation of action execution**.

Well, until now I have mentioned that we should absolutely prevent the agent from taking actions whose results cannot be reversed. An example of such an action is sending an email.

However, nothing prevents, when the agent decides to send an email and even generates its content, from programmatically ending or pausing its activity. In this case, the user can receive information or a dynamic component that allows them to perform this "irreversible" action. Then, confirming its execution can return to the agent and resume its operation, but we fully eliminate the element of "nondeterministic nature."

Such a type of contact assumes, however, that our agent has an interface allowing contact with the user, which will not always be the case. Then, the confirmation itself can take place through **sending an email** or message **on Slack**. Below is an example of a message written based on a link to a movie from the [AI Explained](https://www.youtube.com/@aiexplained-official) channel.

Conversely, if a graphical interface is available, we may consider using solutions like Vercel AI SDK and its **generative UI** functionalities. Below are examples from the site where the assistant's responses not only take the form of text but dynamic components, supplemented with data matched to the query.

Therefore, when designing a system utilizing agency logic, we must ask ourselves the question: **how will the interaction with the user take place**. Specifically:

- **background operation:** task execution happens without human contact, and only the result reaches them. An example could be an agent reacting to events based on which decisions are made about necessary actions
- **asynchronous contact:** task execution does not require ongoing contact with a human, but the task delegation comes from the user. An example could be an agent that can be communicated with via email
- **direct contact:** task execution occurs in collaboration with a human, and information exchange takes place in chat.
- **real-time contact**: it is a variant of direct contact, but in this case, emphasis is placed on low reaction time
- **contact with itself:** the agent can be equipped with a **task planning tool** allowing the setup of a queue of queries, the execution of which may depend on fulfilled conditions (e.g., specific time and location)

But that's not all, because we're not only talking about the overall system assumptions but also details describing the range of responsibilities and permissions of the agent, as well as the user or other agents.

## Summary

Practically **all** previous code examples that appeared in the lesson content can serve as tools for the agent after adaptations are made. In practice, most of them, in a slightly altered form, work daily in my applications and automations.

Ultimately, how the agent's logic is organized and which tools it has access to will depend solely on us. Everything I've shown so far can serve either as a starting point or just an example that you won't apply directly in practice.

However, there are a few very universal rules here that should work in each of your projects. Talking about:

- **Main agent logic** designed in a way that adding or removing tools is not dependent on it. This way, we maintain great flexibility, which facilitates system development.
- **Common data format** that the agent works with. Here, my suggestion involves using documents with the same structure that we used for RAG purposes. This way, we can utilize exactly the same data format **in memory, agent skills, and document processing**, which again offers enormous flexibility.
- **Independence** of tools, allowing their use **beyond the agent's logic**. For example, the `websearch` tool discussed in earlier lessons should be available to us without engaging the agent. This allows for creating simple automations or using the same logic in many places.

At this moment, considering everything we've learned so far about agents, you might consider how your first, simple agent working for you might operate.
Share your idea in the comment.